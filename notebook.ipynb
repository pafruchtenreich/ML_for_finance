{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML for PM & Trading\n",
    "---\n",
    "## Project Proposal: Study of the Links Between Gold Market and US Debt\n",
    "\n",
    "### This project aims to explore the relationships between the gold market, US debt, and other macroeconomic data such as Federal Reserve (FED) rates using machine learning techniques. By analyzing historical data and applying various ML and Statistical models, we hope to uncover patterns and insights that can inform portfolio management and trading strategies.\n",
    "\n",
    "- ``Authors`` : Paul-Antoine FRUCHTENREICH & Bilal BENHANA \n",
    "- ``Date`` : 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API and package init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install kagglehub\n",
    "#pip install seaborn\n",
    "#pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map dataset IDs to their corresponding endpoints\n",
    "\n",
    "dataset_endpoints = {\n",
    "    'debt_to_penny': 'https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v2/accounting/od/debt_to_penny',\n",
    "    'average_interest_rate': 'https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v2/accounting/od/avg_interest_rates',\n",
    "    'tips_cpi': 'https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v1/accounting/od/tips_cpi_data_detail'\n",
    "}\n",
    "\n",
    "def fetch_data(dataset_id, start_date, end_date, sort='record_date'):\n",
    "    \"\"\"\n",
    "    Fetch data from the specified dataset endpoint with optional parameters.\n",
    "\n",
    "    :param dataset_id: str, ID of the dataset to fetch (e.g., 'debt_to_penny', 'average_interest_rate', 'tips_cpi')\n",
    "    :param start_date: str, start date for the data in 'YYYY-MM-DD' format\n",
    "    :param end_date: str, end date for the data in 'YYYY-MM-DD' format\n",
    "    :param sort: str, field to sort the data by (default is 'record_date')\n",
    "    :return: dict, JSON response from the API\n",
    "    \"\"\"\n",
    "    days = (datetime.datetime.strptime(end_date, '%Y-%m-%d') - datetime.datetime.strptime(start_date, '%Y-%m-%d')).days\n",
    "    \n",
    "    if days > 10000:\n",
    "        raise ValueError(f\"The maximum range of data that can be fetched is 10000 days (approximately 27 years). Here, the range is {days} days. (approximately {days//365} years).\")\n",
    "\n",
    "    if dataset_id not in dataset_endpoints:\n",
    "        raise ValueError(f\"Invalid dataset_id. Available options are: {list(dataset_endpoints.keys())}\")\n",
    "\n",
    "    url = dataset_endpoints[dataset_id]\n",
    "    params = {\n",
    "        'filter': f'{sort}:gte:{start_date},{sort}:lte:{end_date}',\n",
    "        'sort': sort,\n",
    "        'page[number]': 1,\n",
    "        'page[size]': days\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "\n",
    "    df = pd.DataFrame(json.loads(response.text)['data'])\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "debt_to_penny_df = fetch_data('debt_to_penny', '2020-01-01', '2021-01-01')\n",
    "average_interest_rate_df = fetch_data('average_interest_rate', '2020-01-01', '2021-01-01')\n",
    "tips_cpi_df = fetch_data('tips_cpi', '1998-01-01', '2020-01-01', sort='original_issue_date')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load gold prices and US debt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the gold price data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download kaggle dataset of gold price from 1979 to 2022 on a daily basis\n",
    "path = kagglehub.dataset_download(\"jishnukoliyadan/gold-price-1979-present\")\n",
    "\n",
    "# The gold with volume, open, high, low, close price  on a daily basis from 1994 to 2021\n",
    "path_2 = kagglehub.dataset_download(\"nward7/gold-historical-datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data = pd.read_csv(path + \"/Daily_Gold_Price_on_World.csv\")\n",
    "gold_data_usd = gold_data[[\"Date\", \"US dollar (USD)\"]]\n",
    "gold_data_usd['Date'] = pd.to_datetime(gold_data_usd['Date'])\n",
    "gold_data_usd = gold_data_usd.rename(columns={\"US dollar (USD)\": \"usd_price\"})\n",
    "gold_data_usd = gold_data_usd.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data_2 = pd.read_csv(path_2 + \"/Gold_Daily .csv\")\n",
    "gold_data_2['Date'] = pd.to_datetime(gold_data_2['Date'])\n",
    "gold_data_2 = gold_data_2.rename(columns={\"Change %\": \"Daily_return\", \"Vol\":\"Volume\"})\n",
    "gold_data_2 = gold_data_2.set_index('Date')\n",
    "# convert the volume to numeric\n",
    "gold_data_2[\"Volume\"] = gold_data_2[\"Volume\"].str.replace(\"K\", \"e3\")\n",
    "gold_data_2[\"Volume\"] = pd.to_numeric(gold_data_2[\"Volume\"], errors='coerce')\n",
    "gold_data_2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load US debt into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching US debt from 1993 to 2023\n",
    "debt_to_penny_df_1 = fetch_data('debt_to_penny', '1993-01-01', '2003-01-01')\n",
    "debt_to_penny_df_2 = fetch_data('debt_to_penny', '2003-01-01', '2023-01-01')\n",
    "debt_to_penny_df = pd.concat([debt_to_penny_df_1, debt_to_penny_df_2])\n",
    "\n",
    "# keep only the relevant columns\n",
    "debt_to_penny_df = debt_to_penny_df[['record_date', 'tot_pub_debt_out_amt']]\n",
    "\n",
    "# convert  tot_pub_debt_out_amt to numeric in trillions\n",
    "debt_to_penny_df['record_date'] = pd.to_datetime(debt_to_penny_df['record_date'])\n",
    "debt_to_penny_df['tot_pub_debt_out_amt'] = pd.to_numeric(debt_to_penny_df['tot_pub_debt_out_amt']) / 1e12  # convert to trillions $\n",
    "debt_to_penny_df = debt_to_penny_df.set_index('record_date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Periods of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some periods of interest\n",
    "decenal_periods = {\n",
    "    '1993-2003': ('1993-01-01', '2003-01-01'),\n",
    "    '2003-2013': ('2003-01-01', '2013-01-01'),\n",
    "    '2013-2023': ('2013-01-01', '2023-01-01')\n",
    "}\n",
    "\n",
    "# Define all the crisis periods since 1993\n",
    "crisis_periods = {\n",
    "    '1997-1998 Asian Financial Crisis': ('1997-07-01', '1998-12-31'),\n",
    "    '2000-2002 Dot-Com Bubble Burst': ('2000-03-01', '2002-12-31'),\n",
    "    '2007-2008 Global Financial Crisis': ('2007-07-01', '2008-12-31'),\n",
    "    '2010-2012 European Debt Crisis': ('2010-01-01', '2012-12-31'),\n",
    "    '2015-2016 Chinese Stock Market Crash': ('2015-06-01', '2016-12-31'),\n",
    "    '2019-2020 COVID-19 Pandemic': ('2019-12-01', '2020-12-31')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute daily returns\n",
    "gold_data_usd['daily_return'] = gold_data_usd['usd_price'].pct_change()\n",
    "debt_to_penny_df['daily_return'] = debt_to_penny_df['tot_pub_debt_out_amt'].pct_change()\n",
    "\n",
    "# compute daily variation\n",
    "gold_data_usd['daily_variation'] = gold_data_usd['usd_price'].diff()\n",
    "debt_to_penny_df['daily_variation'] = debt_to_penny_df['tot_pub_debt_out_amt'].diff()\n",
    "\n",
    "# compute 7d - 28d - 90d - 365 annualized volatility for gold and debt\n",
    "gold_data_usd['volatility_7d'] = gold_data_usd['daily_return'].rolling(window=7).std() * ((365/7)**0.5)\n",
    "gold_data_usd['volatility_28d'] = gold_data_usd['daily_return'].rolling(window=28).std() * ((365/28)**0.5)\n",
    "gold_data_usd['volatility_90d'] = gold_data_usd['daily_return'].rolling(window=90).std() * ((365/90)**0.5)\n",
    "gold_data_usd['volatility_365d'] = gold_data_usd['daily_return'].rolling(window=365).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data_usd['usd_price'].plot(color='gold')\n",
    "plt.title('Gold Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the gold_data_2 price with volume \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Price', color='gold')\n",
    "ax1.plot(gold_data_2.index, gold_data_2['Price'], color='gold')\n",
    "ax1.tick_params(axis='y', labelcolor='gold')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Volume', color='blue')  # we already handled the x-label with ax1\n",
    "ax2.plot(gold_data_2.index, gold_data_2['Volume'], color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('Gold Price and Volume')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debt_to_penny_df['tot_pub_debt_out_amt'].plot()\n",
    "\n",
    "plt.title('US Debt')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Debt in Trillions USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Debt aside Gold Price during some key periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aside(start_date='1979', end_date='2023', crisis_periods=None):\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot gold price data\n",
    "    ax1.plot(gold_data_usd[start_date:end_date].index, gold_data_usd[start_date:end_date]['usd_price'], color='gold', label='Gold Price')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Gold Price (USD)', color='gold')\n",
    "    ax1.tick_params(axis='y', labelcolor='gold')\n",
    "\n",
    "    # Create a second y-axis to plot US debt data\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(debt_to_penny_df[start_date:end_date].index, debt_to_penny_df[start_date:end_date]['tot_pub_debt_out_amt'], color='blue', label='US Debt')\n",
    "    ax2.set_ylabel('US Debt (Trillions USD)', color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Add title and legend\n",
    "    plt.title(f'Gold Price and US Debt during {crisis_periods}')\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for period_name, (start_date, end_date) in decenal_periods.items():\n",
    "    plot_aside(start_date=start_date, end_date=end_date, crisis_periods=period_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that do the bar plot of gold_data_2['Volume'] and line chart of debt_to_penny_df['tot_pub_debt_out_amt'] aside for a given parameter\n",
    "\n",
    "def plot_aside_2(start_date='1979', end_date='2023', crisis_periods=None):\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot gold price data\n",
    "    ax1.bar(gold_data_2[start_date:end_date].index, gold_data_2[start_date:end_date]['Volume'], color='gold', label='Gold Volume')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Gold Volume', color='gold')\n",
    "    ax1.tick_params(axis='y', labelcolor='gold')\n",
    "\n",
    "    # Create a second y-axis to plot US debt data\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(debt_to_penny_df[start_date:end_date].index, debt_to_penny_df[start_date:end_date]['tot_pub_debt_out_amt'], color='blue', label='US Debt')\n",
    "    ax2.set_ylabel('US Debt (Trillions USD)', color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Add title and legend\n",
    "    plt.title(f'Gold Volume and US Debt during {crisis_periods}')\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "for period_name, (start_date, end_date) in decenal_periods.items():\n",
    "    plot_aside_2(start_date=start_date, end_date=end_date, crisis_periods=period_name)\n",
    "\n",
    "for period_name, (start_date, end_date) in crisis_periods.items():\n",
    "    plot_aside_2(start_date=start_date, end_date=end_date, crisis_periods=period_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of retruns for gold and Debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of daily returns for gold and debt\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 7))\n",
    "\n",
    "gold_data_usd['daily_return'].plot.hist(bins=20, ax=ax[0, 0], color='gold')\n",
    "ax[0, 0].set_title('Gold Daily Returns')\n",
    "ax[0, 0].set_xlabel('Daily Return')\n",
    "ax[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "debt_to_penny_df['daily_return'].plot.hist(bins=20, ax=ax[0, 1], color='blue')\n",
    "ax[0, 1].set_title('US Debt Daily Returns')\n",
    "ax[0, 1].set_xlabel('Daily Return')\n",
    "ax[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "gold_data_usd['daily_variation'].plot.hist(bins=20, ax=ax[1, 0], color='gold')\n",
    "ax[1, 0].set_title('Gold Daily Variation')\n",
    "ax[1, 0].set_xlabel('Daily Variation')\n",
    "ax[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "debt_to_penny_df['daily_variation'].plot.hist(bins=20, ax=ax[1, 1], color='blue')\n",
    "ax[1, 1].set_title('US Debt Daily Variation')\n",
    "ax[1, 1].set_xlabel('Daily Variation')\n",
    "ax[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of US Debt Variation and Gold variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Resample the data to daily frequency and fill missing values\n",
    "debt_daily = debt_to_penny_df.resample('D').ffill()\n",
    "\n",
    "# Create a pivot table with years as rows and days of the year as columns\n",
    "debt_daily['Year'] = debt_daily.index.year\n",
    "debt_daily['DayOfYear'] = debt_daily.index.dayofyear\n",
    "# Calculate the daily variation in debt\n",
    "debt_daily['Debt_Variation'] = debt_daily['tot_pub_debt_out_amt'].diff()\n",
    "\n",
    "# Create a pivot table with years as rows and days of the year as columns for debt variation\n",
    "debt_variation_pivot = debt_daily.pivot('Year', 'DayOfYear', 'Debt_Variation')\n",
    "\n",
    "# Plot the heatmap for debt variation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(debt_variation_pivot, center=0, cmap='coolwarm', cbar_kws={'label': 'Debt Variation in  USD'})\n",
    "plt.title('Heatmap of US Debt Variation by Days')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "# Resample the gold data to daily frequency and fill missing values\n",
    "gold_daily = gold_data_usd.resample('D').ffill()\n",
    "\n",
    "# Create a pivot table with years as rows and days of the year as columns\n",
    "gold_daily['Year'] = gold_daily.index.year\n",
    "gold_daily['DayOfYear'] = gold_daily.index.dayofyear\n",
    "# Calculate the daily variation in gold price\n",
    "gold_daily['Gold_Variation'] = gold_daily['usd_price'].diff()\n",
    "\n",
    "# Create a pivot table with years as rows and days of the year as columns for gold price variation\n",
    "gold_variation_pivot = gold_daily.pivot('Year', 'DayOfYear', 'Gold_Variation')\n",
    "\n",
    "# Plot the heatmap for gold price variation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(gold_variation_pivot, center=0, cmap='coolwarm', cbar_kws={'label': 'Gold Price Variation in USD (Trillions)'})\n",
    "plt.title('Heatmap of Gold Price Variation by Days')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap for gold volume \n",
    "gold_daily_2 = gold_data_2.resample('D').ffill()\n",
    "gold_daily_2['Year'] = gold_daily_2.index.year\n",
    "gold_daily_2['DayOfYear'] = gold_daily_2.index.dayofyear\n",
    "gold_daily_2['Gold_Volume'] = gold_data_2['Volume']\n",
    "\n",
    "gold_volume_pivot = gold_daily_2.pivot('Year', 'DayOfYear', 'Gold_Volume')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(gold_volume_pivot, center=0, cmap='coolwarm', cbar_kws={'label': 'Gold Volume'})\n",
    "plt.title('Heatmap of Gold Volume by Days')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of US Daily Return and Gold Daily Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Gold daily returns by year\n",
    "gold_daily['Gold_Return'] = gold_daily['daily_return']\n",
    "gold_return_pivot = gold_daily.pivot('Year', 'DayOfYear', 'Gold_Return')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(gold_return_pivot, cmap='coolwarm', center=0, cbar_kws={'label': 'Gold Daily Return'})\n",
    "plt.title('Heatmap of Gold Daily Returns by Days (1979-2022)')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of Debt daily returns by year\n",
    "debt_daily['Debt_Return'] = debt_daily['daily_return']\n",
    "debt_return_pivot = debt_daily.pivot('Year', 'DayOfYear', 'Debt_Return')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(debt_return_pivot, cmap='coolwarm', center=0, cbar_kws={'label': 'Debt Daily Return'})\n",
    "plt.title('Heatmap of US Debt Daily Returns by Days (1993-2023)')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute correlations between gold prices and US debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(periods):\n",
    "    # Create a DataFrame to store the correlation values\n",
    "    columns = pd.MultiIndex.from_product([[\"gold_vs_debt\"],[\"correlation\",\"correlation_daily_return\"]])\n",
    "    correlations = pd.DataFrame(data=[], index=periods.keys(), columns=columns)\n",
    "\n",
    "    # Calculate and print the correlation for each period\n",
    "    for period, (start_date, end_date) in periods.items():\n",
    "        gold_period = gold_data_usd[start_date:end_date]\n",
    "        debt_period = debt_to_penny_df[start_date:end_date]\n",
    "        \n",
    "        # Merge the dataframes on the date index\n",
    "        merged_data = pd.merge(gold_period, debt_period, left_index=True, right_index=True)\n",
    "        \n",
    "        # Calculate the correlation\n",
    "        correlation = merged_data['usd_price'].corr(merged_data['tot_pub_debt_out_amt'])\n",
    "        correlations.loc[period,(\"gold_vs_debt\",\"correlation\")] = correlation\n",
    "        \n",
    "        # Calculate the correlation of daily returns\n",
    "        correlation_dr = merged_data['daily_return_x'].corr(merged_data['daily_return_y'])\n",
    "        correlations.loc[period,(\"gold_vs_debt\",\"correlation_daily_return\")] = correlation_dr\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "def find_best_correlated_periods(first_date, last_date):\n",
    "    # create a dictionary to store the periods\n",
    "    periods = {}\n",
    "\n",
    "    # Look for the best correlation within first and last_date\n",
    "    for i in range(1, 11):\n",
    "        start_date = pd.to_datetime(first_date)\n",
    "        end_date = start_date + pd.DateOffset(years=i)\n",
    "        if end_date > pd.to_datetime(last_date):\n",
    "            break\n",
    "        periods[f'Period {i}'] = (start_date, end_date)\n",
    "\n",
    "    # Calculate the correlation for each period\n",
    "    correlations = correlation_analysis(periods)\n",
    "    # Find the period with the highest correlation\n",
    "    best_period = correlations.idxmax().values[0][0]\n",
    "\n",
    "    return periods[best_period],correlations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging gold and debt data in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "full_data = gold_data_usd.merge(debt_to_penny_df, how=\"outer\", left_index=True, right_index=True)\n",
    "# rename columns and drop NaN values\n",
    "full_data = full_data.rename(columns={\"tot_pub_debt_out_amt\": \"debt_amt\", \"usd_price\" : \"gold_price\", \"daily_return_x\": \"gold_daily_return\", \"daily_return_y\": \"debt_daily_return\", \"daily_variation_x\": \"gold_daily_variation\", \"daily_variation_y\": \"debt_daily_variation\"}).dropna()\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de Causalité de Granger\n",
    "\n",
    "Le test de causalité de Granger est une méthode statistique utilisée pour déterminer si une série temporelle peut prédire une autre série temporelle. Contrairement à une simple analyse de corrélation, qui mesure seulement la force et la direction de la relation linéaire entre deux variables, le test de Granger examine la capacité d'une variable à fournir des informations sur les valeurs futures d'une autre variable.\n",
    "\n",
    "#### Pertinence par rapport à une analyse de corrélation\n",
    "\n",
    "1. **Directionnalité** : La corrélation ne donne aucune information sur la direction de la relation. Le test de Granger peut indiquer si X cause Y ou si Y cause X.\n",
    "2. **Temporalité** : La corrélation est une mesure instantanée et ne prend pas en compte les décalages temporels. Le test de Granger intègre les décalages temporels pour évaluer la relation causale.\n",
    "3. **Prédiction** : La corrélation ne peut pas être utilisée pour la prédiction, alors que le test de Granger est spécifiquement conçu pour évaluer la capacité prédictive d'une série temporelle sur une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Charger les données\n",
    "data = full_data[['gold_price', 'debt_amt']]\n",
    "\n",
    "# Vérifier la stationnarité des séries temporelles\n",
    "def check_stationarity(series):\n",
    "    result = adfuller(series)\n",
    "    print(f'Statistique ADF: {result[0]}')\n",
    "    print(f'Valeur p: {result[1]}')\n",
    "    print(f'Valeurs critiques: {result[4]}')\n",
    "    return result[1] < 0.05  # Retourne True si la série est stationnaire\n",
    "\n",
    "# Appliquer le test ADF sur les deux séries\n",
    "is_gold_price_stationary = check_stationarity(data['gold_price'])\n",
    "is_us_debt_stationary = check_stationarity(data['debt_amt'])\n",
    "\n",
    "# Différencier les séries si elles ne sont pas stationnaires\n",
    "if not is_gold_price_stationary:\n",
    "    data['gold_price'] = data['gold_price'].diff().dropna()\n",
    "if not is_us_debt_stationary:\n",
    "    data['debt_amt'] = data['debt_amt'].diff().dropna()\n",
    "\n",
    "# Supprimer les valeurs manquantes après différenciation\n",
    "data = data.dropna()\n",
    "\n",
    "# Visualiser les séries temporelles après nettoyage\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(data['gold_price'], label='Prix de l\\'or', color='gold')\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(data['debt_amt'], label='Dette américaine', color='blue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "check_stationarity(data['gold_price'])\n",
    "check_stationarity(data['debt_amt'])\n",
    "\n",
    "# Effectuer le test de causalité de Granger\n",
    "max_lag = 90  # On teste jusqu'à 90 jour de lags (environ 3 mois)\n",
    "model = VAR(data[['gold_price', 'debt_amt']])\n",
    "lag_order = model.select_order(maxlags=max_lag)\n",
    "print(lag_order.summary())\n",
    "\n",
    "# Utiliser le nombre optimal de lags pour le test de Granger\n",
    "optimal_lag = lag_order.aic\n",
    "\n",
    "result_gold_cause_debt = grangercausalitytests(data[['gold_price', 'debt_amt']], optimal_lag, verbose=False) # Test de causalité de l'or sur la dette\n",
    "result_debt_cause_gold = grangercausalitytests(data[['debt_amt', 'gold_price']], optimal_lag, verbose=False) # Test de causalité de la dette sur l'or\n",
    "\n",
    "# Récupérer les p-valeurs des 2 tests de causalité\n",
    "p_values_gold_cause_debt = [result_gold_cause_debt[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "p_values_debt_cause_gold = [result_debt_cause_gold[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "\n",
    "# Visualiser les p-valeurs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_gold_cause_debt, label='Gold causes Debt', color='green')\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_debt_cause_gold, label='Debt causes Gold', color='red')\n",
    "plt.axhline(0.05, color='black', linestyle='--', label='Significance level (0.05)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('Granger Causality Test Results')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'le maxlag optimal est pour les données daily est {optimal_lag} jours soit {optimal_lag//30} mois environ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problème des P-valeurs qui Tendent vers 0\n",
    "\n",
    "Lorsque les p-valeurs descendent à 0 pour des lags élevés dans un test de causalité de Granger, cela peut être dû à plusieurs facteurs :\n",
    "\n",
    "1. **Surajustement (Overfitting)** : Avec un nombre élevé de lags, le modèle peut commencer à surajuster les données, capturant non seulement les relations réelles mais aussi le bruit aléatoire. Cela peut entraîner des p-valeurs artificiellement faibles, indiquant une causalité apparente qui n'est pas réellement présente.\n",
    "\n",
    "2. **Autocorrélation** : Les séries temporelles peuvent avoir une forte autocorrélation, ce qui signifie que les valeurs passées influencent fortement les valeurs futures. Lorsque le nombre de lags est élevé, le modèle peut capturer cette autocorrélation, ce qui peut entraîner des p-valeurs faibles.\n",
    "\n",
    "3. **Dépendance Spurious** : Avec un grand nombre de lags, il est possible que des relations spurious (fausses) soient détectées. Cela peut se produire lorsque le modèle trouve des corrélations qui ne sont pas réellement causales mais qui apparaissent significatives en raison du grand nombre de paramètres estimés.\n",
    "\n",
    "4. **Problèmes de Stationnarité** : Si les séries temporelles ne sont pas correctement stationnarisées, cela peut entraîner des résultats trompeurs. Les séries non stationnaires peuvent montrer des tendances ou des cycles qui peuvent être mal interprétés comme des relations causales.\n",
    "\n",
    "5. **Taille de l'Échantillon** : Avec un grand nombre de lags, le nombre de paramètres à estimer augmente, ce qui peut réduire la puissance statistique du test, surtout si la taille de l'échantillon est limitée. Cela peut également conduire à des p-valeurs faibles de manière artificielle.\n",
    "\n",
    "### Proposition : Agrégation des Données\n",
    "\n",
    "Pour atténuer ces problèmes, une solution consiste à agrégater les données à une fréquence plus faible, comme hebdomadaire (weekly) ou mensuelle (monthly). L'agrégation des données peut aider à :\n",
    "\n",
    "- **Réduire le Bruit** : En lissant les fluctuations quotidiennes, l'agrégation peut réduire le bruit et rendre les tendances sous-jacentes plus visibles.\n",
    "- **Simplifier le Modèle** : Avec des données agrégées, le nombre de lags nécessaires pour capturer les relations temporelles peut être réduit, diminuant ainsi le risque de surajustement.\n",
    "- **Améliorer la Stationnarité** : Les données agrégées peuvent être plus stationnaires, facilitant l'interprétation des résultats du test de Granger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégation hebdomadaire\n",
    "data_weekly = full_data[['gold_price', 'debt_amt']].resample('W').mean()\n",
    "\n",
    "# Différencier les séries si elles ne sont pas stationnaires\n",
    "if not check_stationarity(data_weekly['gold_price']):\n",
    "    data_weekly['gold_price'] = data_weekly['gold_price'].diff().dropna()\n",
    "if not check_stationarity(data_weekly['debt_amt']):\n",
    "    data_weekly['debt_amt'] = data_weekly['debt_amt'].diff().dropna()\n",
    "\n",
    "# Supprimer les valeurs manquantes après différenciation\n",
    "data_weekly = data_weekly.dropna()\n",
    "\n",
    "# Utiliser le critère d'information pour déterminer le nombre optimal de lags\n",
    "max_lag = 90 # On teste jusqu'à 90 semaines de lags (environ 2 ans)\n",
    "model = VAR(data_weekly[['gold_price', 'debt_amt']])\n",
    "lag_order = model.select_order(maxlags=max_lag)\n",
    "optimal_lag = lag_order.aic\n",
    "\n",
    "# Effectuer le test de causalité de Granger avec le nombre optimal de lags\n",
    "result_gold_cause_debt = grangercausalitytests(data_weekly[['gold_price', 'debt_amt']], optimal_lag, verbose=False)\n",
    "result_debt_cause_gold = grangercausalitytests(data_weekly[['debt_amt', 'gold_price']], optimal_lag, verbose=False)\n",
    "\n",
    "# Récupérer les p-valeurs des 2 tests de causalité\n",
    "p_values_gold_cause_debt = [result_gold_cause_debt[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "p_values_debt_cause_gold = [result_debt_cause_gold[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "\n",
    "# Visualiser les p-valeurs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_gold_cause_debt, label='Gold causes Debt', color='green')\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_debt_cause_gold, label='Debt causes Gold', color='red')\n",
    "plt.axhline(0.05, color='black', linestyle='--', label='Significance level (0.05)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('Granger Causality Test Results (Weekly Data)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'le maxlag optimal est pour les donnée hebdomadaire est {optimal_lag} soit environ {optimal_lag//4} mois')\n",
    "print(f'la p-value minimale pour la causalité de l\\'or sur la dette est {min(p_values_gold_cause_debt)} obtenue pour un lag de {p_values_gold_cause_debt.index(min(p_values_gold_cause_debt))} semaines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégation mensuelle\n",
    "\n",
    "data_monthly = full_data[['gold_price', 'debt_amt']].resample('M').mean()\n",
    "\n",
    "# Différencier les séries si elles ne sont pas stationnaires\n",
    "if not check_stationarity(data_monthly['gold_price']):\n",
    "    data_monthly['gold_price'] = data_monthly['gold_price'].diff().dropna()\n",
    "if not check_stationarity(data_monthly['debt_amt']):\n",
    "    data_monthly['debt_amt'] = data_monthly['debt_amt'].diff().dropna()\n",
    "\n",
    "# Supprimer les valeurs manquantes après différenciation\n",
    "data_monthly = data_monthly.dropna()\n",
    "\n",
    "# Utiliser le critère d'information pour déterminer le nombre optimal de lags\n",
    "\n",
    "max_lag = 36  # On teste jusqu'à 36 mois de lags (environ 3 ans)\n",
    "model = VAR(data_monthly[['gold_price', 'debt_amt']])\n",
    "lag_order = model.select_order(maxlags=max_lag)\n",
    "optimal_lag = lag_order.aic\n",
    "\n",
    "# Effectuer le test de causalité de Granger avec le nombre optimal de lags\n",
    "result_gold_cause_debt = grangercausalitytests(data_monthly[['gold_price', 'debt_amt']], optimal_lag, verbose=False)\n",
    "result_debt_cause_gold = grangercausalitytests(data_monthly[['debt_amt', 'gold_price']], optimal_lag, verbose=False)\n",
    "\n",
    "# Récupérer les p-valeurs des 2 tests de causalité\n",
    "p_values_gold_cause_debt = [result_gold_cause_debt[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "p_values_debt_cause_gold = [result_debt_cause_gold[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "\n",
    "# Visualiser les p-valeurs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_gold_cause_debt, label='Gold causes Debt', color='green')\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_debt_cause_gold, label='Debt causes Gold', color='red')\n",
    "plt.axhline(0.05, color='black', linestyle='--', label='Significance level (0.05)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('Granger Causality Test Results (Monthly Data)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Le maxlag optimal pour les données mensuelles est {optimal_lag} mois')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelque soit l'échelle de temps, l'hypothèse G->D semble plus plausible que D->G (cf. interprétation de la p-value)\n",
    "\n",
    "Disclaimer : cette analyse statistique doit être prise comme consultative uniquement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model for Signal Detection\n",
    "\n",
    "Concevoir un modèle ML qui utilise les données du marché de l'or pour obtenir des signaux quant à d'éventuels mouvements non anticipés de la dette américaine pour servir de base à la conception d'un stratégie de trading qui profite de nos prédictions du profil d'endettement US.\n",
    "\n",
    "---\n",
    "#### Steps :\n",
    "\n",
    "- Concevoir un modèle de classification et/ou un modèle de régression.\n",
    "- Concevoir une stratégie de trading basée sur ces modèles et les données du marché de l'or.\n",
    "- Bonus : Concevoir une stratégie de trading basée sur les données de la Fed et du profil d'endettement puis comparer les performances avec la stratégie précédente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "\n",
    "- **Features** : `gold_price`, `gold_volume`, `gold_vol7d`, `gold_vol28d`, `gold_vol90d`, `gold_variation`, `gold_return`\n",
    "- **Target** : Classifier le profil du delta d'endettement à horizon `x` jours/semaines/mois (``remboursement``, ``endettement``, ``surendettement``). Idéalement effectuer une regression pour prédire le delta d'endettement d'ici l'horizon de temps testé.\n",
    "- **Rational** : Le but est de savoir si des patterns particuliers (e.g. : gros volume échangé, changement soudain de volatilité, ou autre) permettent de prédire des accélérations soudaines ou des remboursements anticipés de la dette américaine et si l'or sert vraiment d'indicateur avancé ou bien si les taux/annonces de la Fed et les valeurs passées du profil d'endettement suffisent pour anticiper ces mouvements.\n",
    "- **Hyperparemètre** : \n",
    "    - `x` (l'horizon de temps sur lequel le signal permet au mieux de prédire un surendettement)\n",
    "    - `q` quantile qui definit le seuil s à partir des variations relative de la dette horizon `x` jours\n",
    "    - `s` = `f(q,x)` (le seuil de classification qui permet de discriminer un endettement d'un surendettement) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Chargement des données\n",
    "gold_data = gold_data_2[['Price', 'Volume', 'Daily_return']].rename(columns={'Price': 'gold_price', 'Volume': 'gold_volume', 'Daily_return': 'gold_return'})\n",
    "debt_data = debt_to_penny_df[['tot_pub_debt_out_amt']].rename(columns={'tot_pub_debt_out_amt': 'debt_amt'})\n",
    "\n",
    "# Feature Engineering\n",
    "gold_data['gold_vol7d'] = gold_data['gold_return'].rolling(window=7).std() * ((365/7)**0.5)\n",
    "gold_data['gold_vol28d'] = gold_data['gold_return'].rolling(window=28).std() * ((365/28)**0.5)\n",
    "gold_data['gold_vol90d'] = gold_data['gold_return'].rolling(window=90).std() * ((365/90)**0.5)\n",
    "gold_data['gold_volume_variation'] = gold_data['gold_volume'].diff().fillna(0)\n",
    "gold_data['gold_variation'] = gold_data['gold_price'].diff()\n",
    "gold_data['gold_volume_absolut_variation'] = abs(gold_data['gold_volume_variation'])\n",
    "gold_data['gold_volume_absolut_variation_7d'] = gold_data['gold_volume_absolut_variation'].rolling(window=7).sum()\n",
    "gold_data['gold_volume_absolut_variation_28d'] = gold_data['gold_volume_absolut_variation'].rolling(window=28).sum()\n",
    "gold_data['gold_volume_absolut_variation_90d'] = gold_data['gold_volume_absolut_variation'].rolling(window=90).sum()\n",
    "\n",
    "# Prétraitement des données\n",
    "data = pd.merge(gold_data, debt_data, left_index=True, right_index=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Définition de l'horizon de temps x et du quantile q pour calculer le seuil s\n",
    "x = 30  # par exemple, 30 jours\n",
    "q = 0.95  # par exemple, 95%\n",
    "\n",
    "# Calcul de la variation relative de la dette sur x jours\n",
    "data['debt_delta'] = (data['debt_amt'].shift(-x) - data['debt_amt']) / data['debt_amt']\n",
    "\n",
    "# Computation of s which is the top 5% quantile of the debt_delta distribution playing the role of threshold for the classification of the target variable (remboursement, endettement, surendettement)\n",
    "s = data['debt_delta'].quantile(q)\n",
    "\n",
    "# histogramme de debt_delta\n",
    "data['debt_delta'].hist(bins=50)\n",
    "plt.title(f'Distribution de la variation de la dette horizon {x} jours')\n",
    "plt.xlabel('Variation de la dette')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.show()\n",
    "\n",
    "# count the number of days where the debt_delta is above the threshold\n",
    "print(f'On compte {data[data[\"debt_delta\"] > s].shape[0]} jours sur un total de {data.shape[0]} jours où la variation relative de la dette sur les {x} prochains jours est supérieure au seuil {round(s,2)}')\n",
    "\n",
    "# Classification de la target en 3 classes : remboursement, endettement et surendettement\n",
    "data['target'] = pd.cut(data['debt_delta'], bins=[-np.inf, 0, s, np.inf], labels=['remboursement', 'endettement', 'surendettement'])\n",
    "\n",
    "# Suppression des lignes avec des valeurs manquantes dans la target\n",
    "data.dropna(subset=['target'], inplace=True)\n",
    "\n",
    "# Séparation des données X contenant les features et y contenant la target\n",
    "X = data.drop(columns=['debt_delta', 'target', 'debt_amt'])\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots gold_data and debt_data aside put vertical red line when target is surendettement\n",
    "\n",
    "def plot_aside_3(start_date='1979', end_date='2023', crisis_periods=None):\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot gold price data\n",
    "    ax1.plot(gold_data[start_date:end_date].index, gold_data[start_date:end_date]['gold_price'], color='gold', label='Gold Price')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Gold Price (USD)', color='gold')\n",
    "    ax1.tick_params(axis='y', labelcolor='gold')\n",
    "\n",
    "    # Create a second y-axis to plot US debt data\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(debt_data[start_date:end_date].index, debt_data[start_date:end_date]['debt_amt'], color='blue', label='US Debt')\n",
    "    ax2.set_ylabel('US Debt (Trillions USD)', color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Add title and legend\n",
    "    plt.title(f'Gold Price and US Debt during {crisis_periods}')\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    # Add vertical lines for surendettement if the surrounding data is in the period\n",
    "    surendettement_dates = data[data['target'] == 'surendettement'].index\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    for date in surendettement_dates:\n",
    "        if start_date < date < end_date:\n",
    "            ax1.axvline(date, color='red', linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "for period_name, (start_date, end_date) in crisis_periods.items():\n",
    "    plot_aside_3(start_date=start_date, end_date=end_date, crisis_periods=period_name)\n",
    "\n",
    "for period_name, (start_date, end_date) in decenal_periods.items():\n",
    "    plot_aside_3(start_date=start_date, end_date=end_date, crisis_periods=period_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entraînement d'un modèle de classification de type Gradient Boosting avec GridSearchCV\n",
    "def GBoostTraining(data, selected_features,  param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }):\n",
    "\n",
    "    # Séparation des données X contenant les features et y contenant la target\n",
    "    try:\n",
    "        X = data[selected_features]\n",
    "        y = data['target']\n",
    "    except KeyError:\n",
    "        raise KeyError('Les colonnes sélectionnées ne sont pas présentes dans le jeu de données')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialisation du modèle\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    # Initialisation de GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Prédiction sur les données de test\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Affichage du rapport de classification\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Affichage des meilleurs hyperparamètres\n",
    "    print(f'Les meilleurs hyperparamètres sont: {grid_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "\n",
    "- **Selection des variables** : Ridge Regression\n",
    "- **Optimisation des hyperparamètres et leur impact sur la feature selection** : Une fois qu'on sait entraîner puis tester les performances d'un modèle, il nous reste à chercher les paramètres `x` et `q` optimaux (Cross-Validation) et observer si/comment la feature selection établie par le ridge change en fonction de ``x`` et ``q``\n",
    "- **Redefinition de la target** : la target de classification etait définie par la variation relative de la dette horizon `x` jours mais on pourrait tout à fait imaginer de la definir par rapport à un depassement d'un niveau de volatilité seuil `s` ou variation cumulée pour prendre en compte tous le chemin entre ``t`` et ``t+x``  \n",
    "\n",
    "- **Bonus** : \n",
    "    - **Transformation des données et feature augmentation** : rajouter des features notamment liées à `debt_amt`. Aussi, on peut chercher à transformer nos données (``gold price`` ``inflation-ajusted``, ``debt_amt`` déflaté du ``GDP`` ou GDP/hab)\n",
    "    - **Incorporation des données macroéconomiques** : Ajouter des indicateurs macroéconomiques disponible grâce à l'API tels que le ``taux de chômage``, ``inflation`` (TIPS and CPI), et les ``taux d'intérêt`` pour améliorer les prédictions.\n",
    "    - **Analyse comparative des modèles** : De la classification à la regression (logistique). Tester différents types de boosted trees, puis voire si l'implémentation de RNN comme LSTM pour prendre en compte les relations de dépendence temporelle des time series permet d'ameliorer les performances.\n",
    "    - **Analyse de la saisonnalité** : Étudier les effets saisonniers sur les prix de l'or et la dette américaine -> faire varier l'échelle des données (jusque-là on travaillait avec du daily mais comme on l'a vu dans la partie Granger Causality, il peut être intéressant de revenir à du ``weekly`` ou du ``monthly``) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable selection with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Séparation des données\n",
    "X = data.drop(columns=['debt_delta', 'target', 'debt_amt'])\n",
    "y = data['debt_delta']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Définir les valeurs possibles pour alpha (échelle logarithmique)\n",
    "alphas = np.logspace(-1, 5, 30)\n",
    "\n",
    "# Initialiser le modèle Ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "# Utiliser GridSearchCV pour trouver le meilleur alpha\n",
    "param_grid = {'alpha': alphas}\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Meilleur alpha trouvé\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"Meilleur alpha trouvé: {best_alpha}\")\n",
    "\n",
    "# Entraîner le modèle Ridge avec le meilleur alpha\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Sélectionner les caractéristiques basées sur le modèle Ridge\n",
    "selector = SelectFromModel(ridge_best, prefit=True)\n",
    "X_train_selected = selector.transform(X_train_scaled)\n",
    "\n",
    "# Plot de l'évolution de la significativité des régressions\n",
    "coefs = []\n",
    "for alpha in alphas:\n",
    "    ridge.set_params(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.log10(alphas), coefs)\n",
    "plt.xlabel('log10(alpha)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Evolution des coefficients en fonction de log(alpha)')\n",
    "plt.axis('tight')\n",
    "plt.legend(X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les caractéristiques sélectionnées\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Caractéristiques sélectionnées:\", selected_features)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "y_pred = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "# Calcul de l'erreur quadratique moyenne\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(f\"Erreur quadratique moyenne: {mse}\")\n",
    "\n",
    "# Evalualuation du modèle Ridge\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test.index, y_test, label='True')\n",
    "plt.scatter(y_test.index, y_pred, label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Debt Delta')\n",
    "plt.title('Prédiction du modèle Ridge')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of Boosted Tree with the selected features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des features et de la target\n",
    "X = data[selected_features]\n",
    "y = data['target']\n",
    "\n",
    "# Entraînement d'un modèle de classification de type Gradient Boosting avec GridSearchCV\n",
    "# Séparation des données\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définition des hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialisation du modèle\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialisation de GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Entraînement du modèle\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres\n",
    "print(f'Les meilleurs hyperparamètres sont: {grid_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-Search like function to Fine Tune the model with ``x`` and ``q``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error\n",
    "\n",
    "# Grid-Search like function to Fine Tune the model with x and q\n",
    "param_grid = {\n",
    "    'x': [1, 7, 28, 90],\n",
    "    'q': [0.9, 0.95, .97, 0.99]\n",
    "}\n",
    "\n",
    "def fine_tune_model(param_grid, verbose=False):\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_features = None\n",
    "    for x in param_grid['x']:\n",
    "        for q in param_grid['q']:\n",
    "            # Calcul de la variation relative de la dette sur x jours\n",
    "            data['debt_delta'] = (data['debt_amt'].shift(-x) - data['debt_amt']) / data['debt_amt']\n",
    "\n",
    "            # Calcul du seuil s\n",
    "            s = data['debt_delta'].quantile(q)\n",
    "\n",
    "            # Classification de la target en 3 classes\n",
    "            data['target'] = pd.cut(data['debt_delta'], bins=[-np.inf, 0, s, np.inf], labels=['remboursement', 'endettement', 'surendettement'])\n",
    "\n",
    "            # Suppression des lignes avec des valeurs manquantes dans la target\n",
    "            data.dropna(subset=['target'], inplace=True)\n",
    "\n",
    "            # Séparation des données X contenant les features et y contenant la target\n",
    "            X = data.drop(columns=['debt_delta', 'target', 'debt_amt'])\n",
    "            y = data['debt_delta']\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Regression Ridge pour selectionner les features\n",
    "\n",
    "            # Standardiser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Initialiser le modèle Ridge\n",
    "            ridge = Ridge()\n",
    "\n",
    "            # Utiliser GridSearchCV pour trouver le meilleur alpha\n",
    "            param_grid_ridge = {'alpha': alphas}\n",
    "            grid_search_ridge = GridSearchCV(ridge, param_grid_ridge, cv=5)\n",
    "            grid_search_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Meilleur alpha trouvé\n",
    "            best_alpha = grid_search_ridge.best_params_['alpha']\n",
    "\n",
    "            # Entraîner le modèle Ridge avec le meilleur alpha\n",
    "            ridge_best = Ridge(alpha=best_alpha)\n",
    "            ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Sélectionner les caractéristiques basées sur le modèle Ridge\n",
    "            selector = SelectFromModel(ridge_best, prefit=True)\n",
    "            X_train_selected = selector.transform(X_train_scaled)\n",
    "\n",
    "            # Plot de l'évolution de la significativité des régressions\n",
    "            if verbose:\n",
    "                coefs = []\n",
    "                for alpha in alphas:\n",
    "                    ridge.set_params(alpha=alpha)\n",
    "                    ridge.fit(X_train_scaled, y_train)\n",
    "                    coefs.append(ridge.coef_)\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(np.log10(alphas), coefs)\n",
    "                plt.xlabel('log10(alpha)')\n",
    "                plt.ylabel('Coefficients')\n",
    "                plt.title(f'Evolution des coefficients en fonction de log(alpha) pour x={x} et q={q}')\n",
    "                plt.axis('tight')\n",
    "                plt.legend(X.columns)\n",
    "                plt.show()\n",
    "            \n",
    "            # Afficher les caractéristiques sélectionnées\n",
    "            selected_features = X.columns[selector.get_support()]\n",
    "            if verbose:\n",
    "                print(f'Caractéristiques sélectionnées pour x={x} et q={q}: {selected_features}')\n",
    "\n",
    "            # Evaluation du modèle Ridge\n",
    "            X_test_selected = selector.transform(X_test_scaled)\n",
    "            y_pred = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "            # Compute the score model (here R2 score but can be changed to another metric such as mean_absolute_percentage_error)\n",
    "            score = r2_score(y_test, y_pred)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Score for x={x} and q={q}: {score}')\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'x': x, 'q': q}\n",
    "                best_features = selected_features\n",
    "    \n",
    "    return best_params, best_score, best_features\n",
    "\n",
    "# Fine tune the model\n",
    "\n",
    "best_params, best_score, best_features = fine_tune_model(param_grid, verbose=True)\n",
    "print(f\"Meilleurs paramètres trouvés: {best_params}\")\n",
    "print(f\"Meilleure score: {best_score}\")\n",
    "print(f\"Meilleures caractéristiques sélectionnées: {best_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train a simple regression model with the best parameters\n",
    "\n",
    "x = best_params['x']\n",
    "q = best_params['q']\n",
    "\n",
    "# Prétraitement des données\n",
    "data = pd.merge(gold_data, debt_data, left_index=True, right_index=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Calcul de la variation relative de la dette sur x jours\n",
    "data['debt_delta'] = (data['debt_amt'].shift(-x) - data['debt_amt']) / data['debt_amt']\n",
    "\n",
    "# Suppression des lignes avec des valeurs manquantes dans debt_delta\n",
    "data.dropna(subset=['debt_delta'], inplace=True)\n",
    "\n",
    "# model de regression linéaire avec les features selectionnées\n",
    "X = data[best_features]\n",
    "y = data['debt_delta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# entrainement du modèle de regression linéaire\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcul de l'erreur quadratique moyenne\n",
    "\n",
    "print(f\"Erreur quadratique moyenne: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"Score R2: {r2_score(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error(y_test, y_pred)}\")\n",
    "\n",
    "# Evaluation du modèle de regression linéaire\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test.index, y_test, label='True')\n",
    "plt.scatter(y_test.index, y_pred, label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Debt Delta')\n",
    "plt.title(f'Prédiction du modèle de regression linéaire pour x={x} et q={q}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoostTraining(data, best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Strategy\n",
    "---\n",
    "On a un modèle qui nous permet de detecter des signaux concernant les variations futures (horizon `x` jours) de la dette américaine, il ne nous reste plus qu'à trouver une manière d'en tirer profit avec un stratégie de trading. Deux questions : Quoi trader ? Comment charger les positions ?\n",
    "\n",
    "- **Proxy de la dette américaine** : verfier si on peut encoder la pente de la dette US par le ``spread d'un swap taux US 10y - 1y equi-std`` comme instrument pour notre stratégie de trading\n",
    "- **Backtesting des stratégies de trading** : Mettre en place un environnement de backtesting pour évaluer les performances des stratégies de trading basées sur les modèles prédictifs et sur un proxy de la dette américaine comme instrument tradable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
