{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML for PM & Trading\n",
    "---\n",
    "## Project Proposal: Study of the Links Between Gold Market and US Debt\n",
    "\n",
    "### This project aims to explore the relationships between the gold market, US debt, and other macroeconomic data such as Federal Reserve (FED) rates using machine learning techniques. By analyzing historical data and applying various ML and Statistical models, we hope to uncover patterns and insights that can inform portfolio management and trading strategies.\n",
    "\n",
    "- ``Authors`` : Paul-Antoine FRUCHTENREICH & Bilal BENHANA \n",
    "- ``Date`` : 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API and package init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install kagglehub\n",
    "#pip install seaborn\n",
    "#pip install statsmodels\n",
    "#pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seaborn import heatmap\n",
    "\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import BaseCrossValidator, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, mean_absolute_percentage_error, r2_score, mean_squared_error, confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map dataset IDs to their corresponding endpoints\n",
    "\n",
    "dataset_endpoints = {\n",
    "    'debt_to_penny': 'https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v2/accounting/od/debt_to_penny',\n",
    "    'average_interest_rate': 'https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v2/accounting/od/avg_interest_rates',\n",
    "    'tips_cpi': 'https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v1/accounting/od/tips_cpi_data_detail'\n",
    "}\n",
    "\n",
    "def fetch_data(dataset_id, start_date, end_date, sort='record_date'):\n",
    "    \"\"\"\n",
    "    Fetch data from the specified dataset endpoint with optional parameters.\n",
    "\n",
    "    :param dataset_id: str, ID of the dataset to fetch (e.g., 'debt_to_penny', 'average_interest_rate', 'tips_cpi')\n",
    "    :param start_date: str, start date for the data in 'YYYY-MM-DD' format\n",
    "    :param end_date: str, end date for the data in 'YYYY-MM-DD' format\n",
    "    :param sort: str, field to sort the data by (default is 'record_date')\n",
    "    :return: dict, JSON response from the API\n",
    "    \"\"\"\n",
    "    days = (datetime.datetime.strptime(end_date, '%Y-%m-%d') - datetime.datetime.strptime(start_date, '%Y-%m-%d')).days\n",
    "    \n",
    "    if days > 10000:\n",
    "        raise ValueError(f\"The maximum range of data that can be fetched is 10000 days (approximately 27 years). Here, the range is {days} days. (approximately {days//365} years).\")\n",
    "\n",
    "    if dataset_id not in dataset_endpoints:\n",
    "        raise ValueError(f\"Invalid dataset_id. Available options are: {list(dataset_endpoints.keys())}\")\n",
    "\n",
    "    url = dataset_endpoints[dataset_id]\n",
    "    params = {\n",
    "        'filter': f'{sort}:gte:{start_date},{sort}:lte:{end_date}',\n",
    "        'sort': sort,\n",
    "        'page[number]': 1,\n",
    "        'page[size]': days\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "\n",
    "    df = pd.DataFrame(json.loads(response.text)['data'])\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "debt_to_penny_df = fetch_data('debt_to_penny', '2020-01-01', '2021-01-01')\n",
    "average_interest_rate_df = fetch_data('average_interest_rate', '2020-01-01', '2021-01-01')\n",
    "tips_cpi_df = fetch_data('tips_cpi', '1998-01-01', '2020-01-01', sort='original_issue_date')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load gold prices and US debt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the gold price data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download kaggle dataset of gold price from 1979 to 2022 on a daily basis\n",
    "path = kagglehub.dataset_download(\"jishnukoliyadan/gold-price-1979-present\")\n",
    "\n",
    "# The gold with volume, open, high, low, close price  on a daily basis from 1994 to 2021\n",
    "path_2 = kagglehub.dataset_download(\"nward7/gold-historical-datasets\")\n",
    "\n",
    "# The gold price in USD on a daily basis\n",
    "gold_data = pd.read_csv(path + \"/Daily_Gold_Price_on_World.csv\")\n",
    "gold_data_usd = gold_data[[\"Date\", \"US dollar (USD)\"]]\n",
    "gold_data_usd['Date'] = pd.to_datetime(gold_data_usd['Date'])\n",
    "gold_data_usd = gold_data_usd.rename(columns={\"US dollar (USD)\": \"usd_price\"})\n",
    "gold_data_usd = gold_data_usd.set_index('Date')\n",
    "gold_data_2 = pd.read_csv(path_2 + \"/Gold_Daily .csv\")\n",
    "gold_data_2['Date'] = pd.to_datetime(gold_data_2['Date'])\n",
    "gold_data_2 = gold_data_2.rename(columns={\"Change %\": \"Daily_return\", \"Vol\":\"Volume\"})\n",
    "gold_data_2 = gold_data_2.set_index('Date')\n",
    "# convert the volume to numeric\n",
    "gold_data_2[\"Volume\"] = gold_data_2[\"Volume\"].str.replace(\"K\", \"e3\")\n",
    "gold_data_2[\"Volume\"] = pd.to_numeric(gold_data_2[\"Volume\"], errors='coerce')\n",
    "gold_data_2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load US debt into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching US debt from 1993 to 2023\n",
    "debt_to_penny_df_1 = fetch_data('debt_to_penny', '1993-01-01', '2003-01-01')\n",
    "debt_to_penny_df_2 = fetch_data('debt_to_penny', '2003-01-01', '2023-01-01')\n",
    "debt_to_penny_df = pd.concat([debt_to_penny_df_1, debt_to_penny_df_2])\n",
    "\n",
    "# keep only the relevant columns\n",
    "debt_to_penny_df = debt_to_penny_df[['record_date', 'tot_pub_debt_out_amt']]\n",
    "\n",
    "# convert  tot_pub_debt_out_amt to numeric in trillions\n",
    "debt_to_penny_df['record_date'] = pd.to_datetime(debt_to_penny_df['record_date'])\n",
    "debt_to_penny_df['tot_pub_debt_out_amt'] = pd.to_numeric(debt_to_penny_df['tot_pub_debt_out_amt']) / 1e12  # convert to trillions $\n",
    "debt_to_penny_df = debt_to_penny_df.set_index('record_date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Periods of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some periods of interest\n",
    "decenal_periods = {\n",
    "    '1993-2003': ('1993-01-01', '2003-01-01'),\n",
    "    '2003-2013': ('2003-01-01', '2013-01-01'),\n",
    "    '2013-2023': ('2013-01-01', '2023-01-01')\n",
    "}\n",
    "\n",
    "# Define all the crisis periods since 1993\n",
    "crisis_periods = {\n",
    "    '1997-1998 Asian Financial Crisis': ('1997-07-01', '1998-12-31'),\n",
    "    '2000-2002 Dot-Com Bubble Burst': ('2000-03-01', '2002-12-31'),\n",
    "    '2007-2008 Global Financial Crisis': ('2007-07-01', '2008-12-31'),\n",
    "    '2010-2012 European Debt Crisis': ('2010-01-01', '2012-12-31'),\n",
    "    '2015-2016 Chinese Stock Market Crash': ('2015-06-01', '2016-12-31'),\n",
    "    '2019-2020 COVID-19 Pandemic': ('2019-12-01', '2020-12-31')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load US T-Notes Yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_3 = kagglehub.dataset_download(\"guillemservera/us-treasury-yields-daily\")\n",
    "us_bonds = pd.read_csv(path_3 + \"/us_treasury_yields_daily.csv\")\n",
    "us_bonds[\"date\"] = pd.to_datetime(us_bonds[\"date\"])\n",
    "us_bonds.set_index(\"date\", inplace=True)\n",
    "us_bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute daily returns\n",
    "gold_data_usd['daily_return'] = gold_data_usd['usd_price'].pct_change()\n",
    "debt_to_penny_df['daily_return'] = debt_to_penny_df['tot_pub_debt_out_amt'].pct_change()\n",
    "\n",
    "# compute daily variation\n",
    "gold_data_usd['daily_variation'] = gold_data_usd['usd_price'].diff()\n",
    "debt_to_penny_df['daily_variation'] = debt_to_penny_df['tot_pub_debt_out_amt'].diff()\n",
    "\n",
    "# compute 7d - 28d - 90d - 365 annualized volatility for gold and debt\n",
    "gold_data_usd['volatility_7d'] = gold_data_usd['daily_return'].rolling(window=7).std() * ((365/7)**0.5)\n",
    "gold_data_usd['volatility_28d'] = gold_data_usd['daily_return'].rolling(window=28).std() * ((365/28)**0.5)\n",
    "gold_data_usd['volatility_90d'] = gold_data_usd['daily_return'].rolling(window=90).std() * ((365/90)**0.5)\n",
    "gold_data_usd['volatility_365d'] = gold_data_usd['daily_return'].rolling(window=365).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data_usd['usd_price'].plot(color='gold')\n",
    "plt.title('Gold Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the gold_data_2 price with volume \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Price', color='gold')\n",
    "ax1.plot(gold_data_2.index, gold_data_2['Price'], color='gold')\n",
    "ax1.tick_params(axis='y', labelcolor='gold')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Volume', color='blue')  # we already handled the x-label with ax1\n",
    "ax2.plot(gold_data_2.index, gold_data_2['Volume'], color='blue', alpha=0.7)\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('Gold Price and Volume')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blue too dark -> change opacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debt_to_penny_df['tot_pub_debt_out_amt'].plot()\n",
    "\n",
    "plt.title('US Debt')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Debt in Trillions USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Debt aside Gold Price during some key periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aside(start_date='1979', end_date='2023', crisis_periods=None):\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot gold price data\n",
    "    ax1.plot(gold_data_usd[start_date:end_date].index, gold_data_usd[start_date:end_date]['usd_price'], color='gold', label='Gold Price')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Gold Price (USD)', color='gold')\n",
    "    ax1.tick_params(axis='y', labelcolor='gold')\n",
    "\n",
    "    # Create a second y-axis to plot US debt data\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(debt_to_penny_df[start_date:end_date].index, debt_to_penny_df[start_date:end_date]['tot_pub_debt_out_amt'], color='blue', label='US Debt')\n",
    "    ax2.set_ylabel('US Debt (Trillions USD)', color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Add title and legend\n",
    "    plt.title(f'Gold Price and US Debt during {crisis_periods}')\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for period_name, (start_date, end_date) in decenal_periods.items():\n",
    "    plot_aside(start_date=start_date, end_date=end_date, crisis_periods=period_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that do the bar plot of gold_data_2['Volume'] and line chart of debt_to_penny_df['tot_pub_debt_out_amt'] aside for a given parameter\n",
    "\n",
    "def plot_aside_2(start_date='1979', end_date='2023', crisis_periods=None):\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot gold price data\n",
    "    ax1.bar(gold_data_2[start_date:end_date].index, gold_data_2[start_date:end_date]['Volume'], color='gold', label='Gold Volume')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Gold Volume', color='gold')\n",
    "    ax1.tick_params(axis='y', labelcolor='gold')\n",
    "\n",
    "    # Create a second y-axis to plot US debt data\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(debt_to_penny_df[start_date:end_date].index, debt_to_penny_df[start_date:end_date]['tot_pub_debt_out_amt'], color='blue', label='US Debt')\n",
    "    ax2.set_ylabel('US Debt (Trillions USD)', color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Add title and legend\n",
    "    plt.title(f'Gold Volume and US Debt during {crisis_periods}')\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "for period_name, (start_date, end_date) in decenal_periods.items():\n",
    "    plot_aside_2(start_date=start_date, end_date=end_date, crisis_periods=period_name)\n",
    "\n",
    "for period_name, (start_date, end_date) in crisis_periods.items():\n",
    "    plot_aside_2(start_date=start_date, end_date=end_date, crisis_periods=period_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpréter les pics très réguliers du volume de l'or, check last trading date pour expliquer les pics réguliers, fin de mois ? fin de semestre ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of retruns for gold and Debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of daily returns for gold and debt\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 7))\n",
    "\n",
    "gold_data_usd['daily_return'].plot.hist(bins=20, ax=ax[0, 0], color='gold')\n",
    "ax[0, 0].set_title('Gold Daily Returns')\n",
    "ax[0, 0].set_xlabel('Daily Return')\n",
    "ax[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "debt_to_penny_df['daily_return'].plot.hist(bins=20, ax=ax[0, 1], color='blue')\n",
    "ax[0, 1].set_title('US Debt Daily Returns')\n",
    "ax[0, 1].set_xlabel('Daily Return')\n",
    "ax[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "gold_data_usd['daily_variation'].plot.hist(bins=20, ax=ax[1, 0], color='gold')\n",
    "ax[1, 0].set_title('Gold Daily Variation')\n",
    "ax[1, 0].set_xlabel('Daily Variation')\n",
    "ax[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "debt_to_penny_df['daily_variation'].plot.hist(bins=20, ax=ax[1, 1], color='blue')\n",
    "ax[1, 1].set_title('US Debt Daily Variation')\n",
    "ax[1, 1].set_xlabel('Daily Variation')\n",
    "ax[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enlevable imo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of US Debt Variation and Gold variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Resample the data to daily frequency and fill missing values\n",
    "debt_daily = debt_to_penny_df.resample('D').ffill()\n",
    "\n",
    "# Create a pivot table with years as rows and days of the year as columns\n",
    "debt_daily['Year'] = debt_daily.index.year\n",
    "debt_daily['DayOfYear'] = debt_daily.index.dayofyear\n",
    "# Calculate the daily variation in debt\n",
    "debt_daily['Debt_Variation'] = debt_daily['tot_pub_debt_out_amt'].diff()\n",
    "\n",
    "# Create a pivot table with years as rows and days of the year as columns for debt variation\n",
    "debt_variation_pivot = debt_daily.pivot('Year', 'DayOfYear', 'Debt_Variation')\n",
    "\n",
    "# Plot the heatmap for debt variation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(debt_variation_pivot, center=0, cmap='coolwarm', cbar_kws={'label': 'Debt Variation in  USD'})\n",
    "plt.title('Heatmap of US Debt Variation by Days')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "# Resample the gold data to daily frequency and fill missing values\n",
    "gold_daily = gold_data_usd.resample('D').ffill()\n",
    "\n",
    "# Create a pivot table with years as rows and days of the year as columns\n",
    "gold_daily['Year'] = gold_daily.index.year\n",
    "gold_daily['DayOfYear'] = gold_daily.index.dayofyear\n",
    "# Calculate the daily variation in gold price\n",
    "gold_daily['Gold_Variation'] = gold_daily['usd_price'].diff()\n",
    "\n",
    "# Create a pivot table with years as rows and days of the year as columns for gold price variation\n",
    "gold_variation_pivot = gold_daily.pivot('Year', 'DayOfYear', 'Gold_Variation')\n",
    "\n",
    "# Plot the heatmap for gold price variation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(gold_variation_pivot, center=0, cmap='coolwarm', cbar_kws={'label': 'Gold Price Variation in USD (Trillions)'})\n",
    "plt.title('Heatmap of Gold Price Variation by Days')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap for gold volume \n",
    "gold_daily_2 = gold_data_2.resample('D').ffill()\n",
    "gold_daily_2['Year'] = gold_daily_2.index.year\n",
    "gold_daily_2['DayOfYear'] = gold_daily_2.index.dayofyear\n",
    "gold_daily_2['Gold_Volume'] = gold_data_2['Volume']\n",
    "\n",
    "gold_volume_pivot = gold_daily_2.pivot('Year', 'DayOfYear', 'Gold_Volume')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(gold_volume_pivot, center=0, cmap='coolwarm', cbar_kws={'label': 'Gold Volume'})\n",
    "plt.title('Heatmap of Gold Volume by Days')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of US Daily Return and Gold Daily Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Gold daily returns by year\n",
    "gold_daily['Gold_Return'] = gold_daily['daily_return']\n",
    "gold_return_pivot = gold_daily.pivot('Year', 'DayOfYear', 'Gold_Return')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(gold_return_pivot, cmap='coolwarm', center=0, cbar_kws={'label': 'Gold Daily Return'})\n",
    "plt.title('Heatmap of Gold Daily Returns by Days (1979-2022)')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of Debt daily returns by year\n",
    "debt_daily['Debt_Return'] = debt_daily['daily_return']\n",
    "debt_return_pivot = debt_daily.pivot('Year', 'DayOfYear', 'Debt_Return')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(debt_return_pivot, cmap='coolwarm', center=0, cbar_kws={'label': 'Debt Daily Return'})\n",
    "plt.title('Heatmap of US Debt Daily Returns by Days (1993-2023)')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute correlations between gold prices and US debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(periods):\n",
    "    # Create a DataFrame to store the correlation values\n",
    "    columns = pd.MultiIndex.from_product([[\"gold_vs_debt\"],[\"correlation\",\"correlation_daily_return\"]])\n",
    "    correlations = pd.DataFrame(data=[], index=periods.keys(), columns=columns)\n",
    "\n",
    "    # Calculate and print the correlation for each period\n",
    "    for period, (start_date, end_date) in periods.items():\n",
    "        gold_period = gold_data_usd[start_date:end_date]\n",
    "        debt_period = debt_to_penny_df[start_date:end_date]\n",
    "        \n",
    "        # Merge the dataframes on the date index\n",
    "        merged_data = pd.merge(gold_period, debt_period, left_index=True, right_index=True)\n",
    "        \n",
    "        # Calculate the correlation\n",
    "        correlation = merged_data['usd_price'].corr(merged_data['tot_pub_debt_out_amt'])\n",
    "        correlations.loc[period,(\"gold_vs_debt\",\"correlation\")] = correlation\n",
    "        \n",
    "        # Calculate the correlation of daily returns\n",
    "        correlation_dr = merged_data['daily_return_x'].corr(merged_data['daily_return_y'])\n",
    "        correlations.loc[period,(\"gold_vs_debt\",\"correlation_daily_return\")] = correlation_dr\n",
    "        \n",
    "    return correlations\n",
    "\n",
    "def find_best_correlated_periods(first_date, last_date):\n",
    "    # create a dictionary to store the periods\n",
    "    periods = {}\n",
    "\n",
    "    # Look for the best correlation within first and last_date\n",
    "    for i in range(1, 11):\n",
    "        start_date = pd.to_datetime(first_date)\n",
    "        end_date = start_date + pd.DateOffset(years=i)\n",
    "        if end_date > pd.to_datetime(last_date):\n",
    "            break\n",
    "        periods[f'Period {i}'] = (start_date, end_date)\n",
    "\n",
    "    # Calculate the correlation for each period\n",
    "    correlations = correlation_analysis(periods)\n",
    "    # Find the period with the highest correlation\n",
    "    best_period = correlations.idxmax().values[0][0]\n",
    "\n",
    "    return periods[best_period],correlations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging gold and debt data in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "full_data = gold_data_usd.merge(debt_to_penny_df, how=\"outer\", left_index=True, right_index=True)\n",
    "# rename columns and drop NaN values\n",
    "full_data = full_data.rename(columns={\"tot_pub_debt_out_amt\": \"debt_amt\", \"usd_price\" : \"gold_price\", \"daily_return_x\": \"gold_daily_return\", \"daily_return_y\": \"debt_daily_return\", \"daily_variation_x\": \"gold_daily_variation\", \"daily_variation_y\": \"debt_daily_variation\"}).dropna()\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de Causalité de Granger\n",
    "\n",
    "Le test de causalité de Granger est une méthode statistique utilisée pour déterminer si une série temporelle peut prédire une autre série temporelle. Contrairement à une simple analyse de corrélation, qui mesure seulement la force et la direction de la relation linéaire entre deux variables, le test de Granger examine la capacité d'une variable à fournir des informations sur les valeurs futures d'une autre variable.\n",
    "\n",
    "#### Pertinence par rapport à une analyse de corrélation\n",
    "\n",
    "1. **Directionnalité** : La corrélation ne donne aucune information sur la direction de la relation. Le test de Granger peut indiquer si X cause Y ou si Y cause X.\n",
    "2. **Temporalité** : La corrélation est une mesure instantanée et ne prend pas en compte les décalages temporels. Le test de Granger intègre les décalages temporels pour évaluer la relation causale.\n",
    "3. **Prédiction** : La corrélation ne peut pas être utilisée pour la prédiction, alors que le test de Granger est spécifiquement conçu pour évaluer la capacité prédictive d'une série temporelle sur une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "data = full_data[['gold_price', 'debt_amt']]\n",
    "\n",
    "# Vérifier la stationnarité des séries temporelles\n",
    "def check_stationarity(series):\n",
    "    result = adfuller(series)\n",
    "    print(f'Statistique ADF: {result[0]}')\n",
    "    print(f'Valeur p: {result[1]}')\n",
    "    print(f'Valeurs critiques: {result[4]}')\n",
    "    return result[1] < 0.05  # Retourne True si la série est stationnaire\n",
    "\n",
    "# Appliquer le test ADF sur les deux séries\n",
    "is_gold_price_stationary = check_stationarity(data['gold_price'])\n",
    "is_us_debt_stationary = check_stationarity(data['debt_amt'])\n",
    "\n",
    "# Différencier les séries si elles ne sont pas stationnaires\n",
    "if not is_gold_price_stationary:\n",
    "    data['gold_price'] = data['gold_price'].diff().dropna()\n",
    "if not is_us_debt_stationary:\n",
    "    data['debt_amt'] = data['debt_amt'].diff().dropna()\n",
    "\n",
    "# Supprimer les valeurs manquantes après différenciation\n",
    "data = data.dropna()\n",
    "\n",
    "# Visualiser les séries temporelles après nettoyage\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(data['gold_price'], label='Prix de l\\'or', color='gold')\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(data['debt_amt'], label='Dette américaine', color='blue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "check_stationarity(data['gold_price'])\n",
    "check_stationarity(data['debt_amt'])\n",
    "\n",
    "# Effectuer le test de causalité de Granger\n",
    "max_lag = 90  # On teste jusqu'à 90 jour de lags (environ 3 mois)\n",
    "model = VAR(data[['gold_price', 'debt_amt']])\n",
    "lag_order = model.select_order(maxlags=max_lag)\n",
    "print(lag_order.summary())\n",
    "\n",
    "# Utiliser le nombre optimal de lags pour le test de Granger\n",
    "optimal_lag = lag_order.aic\n",
    "\n",
    "result_gold_cause_debt = grangercausalitytests(data[['gold_price', 'debt_amt']], optimal_lag, verbose=False) # Test de causalité de l'or sur la dette\n",
    "result_debt_cause_gold = grangercausalitytests(data[['debt_amt', 'gold_price']], optimal_lag, verbose=False) # Test de causalité de la dette sur l'or\n",
    "\n",
    "# Récupérer les p-valeurs des 2 tests de causalité\n",
    "p_values_gold_cause_debt = [result_gold_cause_debt[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "p_values_debt_cause_gold = [result_debt_cause_gold[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "\n",
    "# Visualiser les p-valeurs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_gold_cause_debt, label='Gold causes Debt', color='green')\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_debt_cause_gold, label='Debt causes Gold', color='red')\n",
    "plt.axhline(0.05, color='black', linestyle='--', label='Significance level (0.05)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('Granger Causality Test Results')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'le maxlag optimal est pour les données daily est {optimal_lag} jours soit {optimal_lag//30} mois environ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problème des P-valeurs qui Tendent vers 0\n",
    "\n",
    "Lorsque les p-valeurs descendent à 0 pour des lags élevés dans un test de causalité de Granger, cela peut être dû à plusieurs facteurs :\n",
    "\n",
    "1. **Surajustement (Overfitting)** : Avec un nombre élevé de lags, le modèle peut commencer à surajuster les données, capturant non seulement les relations réelles mais aussi le bruit aléatoire. Cela peut entraîner des p-valeurs artificiellement faibles, indiquant une causalité apparente qui n'est pas réellement présente.\n",
    "\n",
    "2. **Autocorrélation** : Les séries temporelles peuvent avoir une forte autocorrélation, ce qui signifie que les valeurs passées influencent fortement les valeurs futures. Lorsque le nombre de lags est élevé, le modèle peut capturer cette autocorrélation, ce qui peut entraîner des p-valeurs faibles.\n",
    "\n",
    "3. **Dépendance Spurious** : Avec un grand nombre de lags, il est possible que des relations spurious (fausses) soient détectées. Cela peut se produire lorsque le modèle trouve des corrélations qui ne sont pas réellement causales mais qui apparaissent significatives en raison du grand nombre de paramètres estimés.\n",
    "\n",
    "4. **Problèmes de Stationnarité** : Si les séries temporelles ne sont pas correctement stationnarisées, cela peut entraîner des résultats trompeurs. Les séries non stationnaires peuvent montrer des tendances ou des cycles qui peuvent être mal interprétés comme des relations causales.\n",
    "\n",
    "5. **Taille de l'Échantillon** : Avec un grand nombre de lags, le nombre de paramètres à estimer augmente, ce qui peut réduire la puissance statistique du test, surtout si la taille de l'échantillon est limitée. Cela peut également conduire à des p-valeurs faibles de manière artificielle.\n",
    "\n",
    "### Proposition : Agrégation des Données\n",
    "\n",
    "Pour atténuer ces problèmes, une solution consiste à agrégater les données à une fréquence plus faible, comme hebdomadaire (weekly) ou mensuelle (monthly). L'agrégation des données peut aider à :\n",
    "\n",
    "- **Réduire le Bruit** : En lissant les fluctuations quotidiennes, l'agrégation peut réduire le bruit et rendre les tendances sous-jacentes plus visibles.\n",
    "- **Simplifier le Modèle** : Avec des données agrégées, le nombre de lags nécessaires pour capturer les relations temporelles peut être réduit, diminuant ainsi le risque de surajustement.\n",
    "- **Améliorer la Stationnarité** : Les données agrégées peuvent être plus stationnaires, facilitant l'interprétation des résultats du test de Granger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégation hebdomadaire\n",
    "data_weekly = full_data[['gold_price', 'debt_amt']].resample('W').mean()\n",
    "\n",
    "# Différencier les séries si elles ne sont pas stationnaires\n",
    "if not check_stationarity(data_weekly['gold_price']):\n",
    "    data_weekly['gold_price'] = data_weekly['gold_price'].diff().dropna()\n",
    "if not check_stationarity(data_weekly['debt_amt']):\n",
    "    data_weekly['debt_amt'] = data_weekly['debt_amt'].diff().dropna()\n",
    "\n",
    "# Supprimer les valeurs manquantes après différenciation\n",
    "data_weekly = data_weekly.dropna()\n",
    "\n",
    "# Utiliser le critère d'information pour déterminer le nombre optimal de lags\n",
    "max_lag = 90 # On teste jusqu'à 90 semaines de lags (environ 2 ans)\n",
    "model = VAR(data_weekly[['gold_price', 'debt_amt']])\n",
    "lag_order = model.select_order(maxlags=max_lag)\n",
    "optimal_lag = lag_order.aic\n",
    "\n",
    "# Effectuer le test de causalité de Granger avec le nombre optimal de lags\n",
    "result_gold_cause_debt = grangercausalitytests(data_weekly[['gold_price', 'debt_amt']], optimal_lag, verbose=False)\n",
    "result_debt_cause_gold = grangercausalitytests(data_weekly[['debt_amt', 'gold_price']], optimal_lag, verbose=False)\n",
    "\n",
    "# Récupérer les p-valeurs des 2 tests de causalité\n",
    "p_values_gold_cause_debt = [result_gold_cause_debt[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "p_values_debt_cause_gold = [result_debt_cause_gold[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "\n",
    "# Visualiser les p-valeurs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_gold_cause_debt, label='Gold causes Debt', color='green')\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_debt_cause_gold, label='Debt causes Gold', color='red')\n",
    "plt.axhline(0.05, color='black', linestyle='--', label='Significance level (0.05)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('Granger Causality Test Results (Weekly Data)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'le maxlag optimal est pour les données hebdomadaires est {optimal_lag} soit environ {optimal_lag//4} mois')\n",
    "print(f'la p-value minimale pour la causalité de l\\'or sur la dette est {min(p_values_gold_cause_debt)} obtenue pour un lag de {p_values_gold_cause_debt.index(min(p_values_gold_cause_debt))} semaines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégation mensuelle\n",
    "\n",
    "data_monthly = full_data[['gold_price', 'debt_amt']].resample('M').mean()\n",
    "\n",
    "# Différencier les séries si elles ne sont pas stationnaires\n",
    "if not check_stationarity(data_monthly['gold_price']):\n",
    "    data_monthly['gold_price'] = data_monthly['gold_price'].diff().dropna()\n",
    "if not check_stationarity(data_monthly['debt_amt']):\n",
    "    data_monthly['debt_amt'] = data_monthly['debt_amt'].diff().dropna()\n",
    "\n",
    "# Supprimer les valeurs manquantes après différenciation\n",
    "data_monthly = data_monthly.dropna()\n",
    "\n",
    "# Utiliser le critère d'information pour déterminer le nombre optimal de lags\n",
    "\n",
    "max_lag = 36  # On teste jusqu'à 36 mois de lags (environ 3 ans)\n",
    "model = VAR(data_monthly[['gold_price', 'debt_amt']])\n",
    "lag_order = model.select_order(maxlags=max_lag)\n",
    "optimal_lag = lag_order.aic\n",
    "\n",
    "# Effectuer le test de causalité de Granger avec le nombre optimal de lags\n",
    "result_gold_cause_debt = grangercausalitytests(data_monthly[['gold_price', 'debt_amt']], optimal_lag, verbose=False)\n",
    "result_debt_cause_gold = grangercausalitytests(data_monthly[['debt_amt', 'gold_price']], optimal_lag, verbose=False)\n",
    "\n",
    "# Récupérer les p-valeurs des 2 tests de causalité\n",
    "p_values_gold_cause_debt = [result_gold_cause_debt[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "p_values_debt_cause_gold = [result_debt_cause_gold[i + 1][0]['ssr_ftest'][1] for i in range(optimal_lag)]\n",
    "\n",
    "# Visualiser les p-valeurs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_gold_cause_debt, label='Gold causes Debt', color='green')\n",
    "plt.plot(range(1, optimal_lag + 1), p_values_debt_cause_gold, label='Debt causes Gold', color='red')\n",
    "plt.axhline(0.05, color='black', linestyle='--', label='Significance level (0.05)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('Granger Causality Test Results (Monthly Data)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Le maxlag optimal pour les données mensuelles est {optimal_lag} mois')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelque soit l'échelle de temps, l'hypothèse G->D semble plus plausible que D->G (cf. interprétation de la p-value)\n",
    "\n",
    "Disclaimer : cette analyse statistique doit être prise comme consultative uniquement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model for Signal Detection\n",
    "\n",
    "Concevoir un modèle ML qui utilise les données du marché de l'or pour obtenir des signaux quant à d'éventuels mouvements non anticipés de la dette américaine pour servir de base à la conception d'un stratégie de trading qui profite de nos prédictions du profil d'endettement US.\n",
    "\n",
    "---\n",
    "#### Steps :\n",
    "\n",
    "- Concevoir un modèle de classification et/ou un modèle de régression.\n",
    "- Concevoir une stratégie de trading basée sur ces modèles et les données du marché de l'or.\n",
    "- Bonus : Concevoir une stratégie de trading basée sur les données de la Fed et du profil d'endettement puis comparer les performances avec la stratégie précédente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "\n",
    "- **Features** : `gold_price`, `gold_volume`, `gold_vol7d`, `gold_vol28d`, `gold_vol90d`, `gold_variation`, `gold_return`\n",
    "- **Target** : Classifier le profil du delta d'endettement à horizon `x` jours/semaines/mois (``remboursement``, ``endettement``, ``surendettement``). Idéalement effectuer une regression pour prédire le delta d'endettement d'ici l'horizon de temps testé.\n",
    "- **Rational** : Le but est de savoir si des patterns particuliers (e.g. : gros volume échangé, changement soudain de volatilité, ou autre) permettent de prédire des accélérations soudaines ou des remboursements anticipés de la dette américaine et si l'or sert vraiment d'indicateur avancé ou bien si les taux/annonces de la Fed et les valeurs passées du profil d'endettement suffisent pour anticiper ces mouvements.\n",
    "- **Hyperparemètre** : \n",
    "    - `x` (l'horizon de temps sur lequel le signal permet au mieux de prédire un surendettement)\n",
    "    - `q` quantile qui definit le seuil s à partir des variations relative de la dette horizon `x` jours\n",
    "    - `s` = `f(q,x)` (le seuil de classification qui permet de discriminer un endettement d'un surendettement) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "gold_data = gold_data_2[['Price', 'Volume', 'Daily_return']].rename(columns={'Price': 'gold_price', 'Volume': 'gold_volume', 'Daily_return': 'gold_return'})\n",
    "debt_data = debt_to_penny_df[['tot_pub_debt_out_amt']].rename(columns={'tot_pub_debt_out_amt': 'debt_amt'})\n",
    "\n",
    "# Feature Engineering\n",
    "gold_data['gold_vol7d'] = gold_data['gold_return'].rolling(window=7).std() * ((365/7)**0.5)\n",
    "gold_data['gold_vol28d'] = gold_data['gold_return'].rolling(window=28).std() * ((365/28)**0.5)\n",
    "gold_data['gold_vol90d'] = gold_data['gold_return'].rolling(window=90).std() * ((365/90)**0.5)\n",
    "gold_data['gold_volume_variation'] = gold_data['gold_volume'].diff().fillna(0)\n",
    "gold_data['gold_variation'] = gold_data['gold_price'].diff()\n",
    "gold_data['gold_volume_absolut_variation'] = abs(gold_data['gold_volume_variation'])\n",
    "gold_data['gold_volume_absolut_variation_7d'] = gold_data['gold_volume_absolut_variation'].rolling(window=7).sum()\n",
    "gold_data['gold_volume_absolut_variation_28d'] = gold_data['gold_volume_absolut_variation'].rolling(window=28).sum()\n",
    "gold_data['gold_volume_absolut_variation_90d'] = gold_data['gold_volume_absolut_variation'].rolling(window=90).sum()\n",
    "\n",
    "# Prétraitement des données\n",
    "data = pd.merge(gold_data, debt_data, left_index=True, right_index=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Target Engineering\n",
    "class DebtDeltaTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, x, q):\n",
    "        self.x = x\n",
    "        self.q = q\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Calcul de la variation relative de la dette sur x jours\n",
    "        X['debt_delta'] = (X['debt_amt'].shift(-self.x) - X['debt_amt']) / X['debt_amt']\n",
    "        # Computation of s which is the top q quantile of the debt_delta distribution playing the role of threshold for the classification of the target variable (remboursement, endettement, surendettement)\n",
    "        s = X['debt_delta'].quantile(self.q)\n",
    "        X['target'] = pd.cut(X['debt_delta'], bins=[-np.inf, 0, s, np.inf], labels=['remboursement', 'endettement', 'surendettement'])\n",
    "        X.dropna(subset=['target'], inplace=True)\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of the target characteristics for x,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_target_characteristics(data, x=30, q=0.95, periods=decenal_periods):\n",
    "    data = DebtDeltaTransformer(x, q).transform(data)\n",
    "    \n",
    "    s = data['debt_delta'].quantile(q)\n",
    "\n",
    "    # histogramme de debt_delta\n",
    "    data['debt_delta'].hist(bins=50)\n",
    "    plt.title(f'Distribution de la variation de la dette horizon {x} jours')\n",
    "    plt.xlabel('Variation de la dette')\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.show()\n",
    "\n",
    "    # count the number of days where the debt_delta is above the threshold\n",
    "    print(f'On compte {data[data[\"debt_delta\"] > s].shape[0]} jours sur un total de {data.shape[0]} jours où la variation relative de la dette sur les {x} prochains jours est supérieure au seuil {round(s,2)}')\n",
    "\n",
    "    # Classification de la target en 3 classes : remboursement, endettement et surendettement\n",
    "    data['target'] = pd.cut(data['debt_delta'], bins=[-np.inf, 0, s, np.inf], labels=['remboursement', 'endettement', 'surendettement'])\n",
    "\n",
    "    # Suppression des lignes avec des valeurs manquantes dans la target\n",
    "    data.dropna(subset=['target'], inplace=True)\n",
    "    # Plots gold_data and debt_data aside put vertical red line when target is surendettement\n",
    "\n",
    "    def plot_aside_3(start_date='1979', end_date='2023', crisis_periods=None):\n",
    "        fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "        # Plot gold price data\n",
    "        ax1.plot(gold_data[start_date:end_date].index, gold_data[start_date:end_date]['gold_price'], color='gold', label='Gold Price')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Gold Price (USD)', color='gold')\n",
    "        ax1.tick_params(axis='y', labelcolor='gold')\n",
    "\n",
    "        # Create a second y-axis to plot US debt data\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(debt_data[start_date:end_date].index, debt_data[start_date:end_date]['debt_amt'], color='blue', label='US Debt')\n",
    "        ax2.set_ylabel('US Debt (Trillions USD)', color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        # Add title and legend\n",
    "        plt.title(f'Gold Price and US Debt during {crisis_periods}')\n",
    "        fig.tight_layout()\n",
    "        fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "        # Add vertical lines for surendettement if the surrounding data is in the period\n",
    "        surendettement_dates = data[data['target'] == 'surendettement'].index\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        for date in surendettement_dates:\n",
    "            if start_date < date < end_date:\n",
    "                ax1.axvline(date, color='red', linestyle='--')\n",
    "        plt.show()\n",
    "    \n",
    "    for period_name, (start_date, end_date) in periods.items():\n",
    "        plot_aside_3(start_date=start_date, end_date=end_date, crisis_periods=period_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_characteristics(data, x=30, q=0.95, periods=decenal_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla Model\n",
    "- We try to predict the `target` classification of incoming `debt_delta` only with `gold` data\n",
    "- Vanilla = without optimization over x,q and without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Pipeline for Target Classification with Scaler, Transform (debt_amt -> debt_delta -> target) and Classifier\n",
    "\n",
    "def Vanilla_Classifier(x,q, data,  param_grid={'classifier__n_estimators': [100, 200],'classifier__learning_rate': [0.01, 0.1]}):\n",
    "    # Target creation\n",
    "    data = DebtDeltaTransformer(x, q).transform(data)\n",
    "\n",
    "    # Pipeline for the classification task\n",
    "    model_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', GradientBoostingClassifier())\n",
    "    ])\n",
    "\n",
    "    # Initialiser GridSearchCV avec TimeSeriesSplit\n",
    "    grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=TimeSeriesSplit(n_splits=5))\n",
    "\n",
    "    # Ajuster GridSearchCV aux données\n",
    "    grid_search.fit(data.drop(columns=['target']), data['target'])\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention le code ci dessus est faux car il train sur toute les données j'ai essayé avec un train_test_split de renvoyé en plus du modele trained les donnée test mais ça bug donc à fixer\n",
    "voir si ça aide : https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for a given (x,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_vanilla_model(x, q, data, verbose=False):\n",
    "    \n",
    "    model, X_test, y_test = Vanilla_Classifier(x, q, data)\n",
    "\n",
    "    # Prédiction des classes de la target\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if verbose:\n",
    "        # Affichage du rapport de classification\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Matrice de confusion\n",
    "        confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Matrice de confusion')\n",
    "        plt.show()\n",
    "\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "evaluate_vanilla_model(30, 0.95, data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "\n",
    "- **Selection des variables** : ``Lasso``, ``Ridge``, ``Elastic Net`` or ``SelectKBest`` (need to do a CV for best ``alpha`` or best ``K``)\n",
    "- **Optimisation des hyperparamètres et leur impact sur la feature selection** : Une fois qu'on sait entraîner puis tester les performances d'un modèle, il nous reste à chercher les paramètres `x` et `q` optimaux (Cross-Validation) et observer si/comment la feature selection établie par le ridge change en fonction de ``x`` et ``q``\n",
    "- **Redefinition de la target** : la target de classification etait définie par la variation relative de la dette horizon `x` jours mais on pourrait tout à fait imaginer de la definir par rapport à un depassement d'un niveau de volatilité seuil `s` ou variation cumulée pour prendre en compte tous le chemin entre ``t`` et ``t+x``  \n",
    "- **Optimiser la ``classification_threshold``** :  selon si on prefere une precision elevé ou un recall élevé, par rapport au fait que le cout de faire l'erreur de classifier un surendettement en remboursement était pire que toute autre missclassfication car les variations relatives aux surendettement étaient plus violentes que les variation relatives aux remboursement (https://scikit-learn.org/stable/modules/classification_threshold.html)\n",
    "\n",
    "- **Bonus** : \n",
    "    - **Transformation des données et feature augmentation** : rajouter des features notamment liées à `debt_amt`. Aussi, on peut chercher à transformer nos données (``gold price`` ``inflation-ajusted``, ``debt_amt`` déflaté du ``GDP`` ou GDP/hab)\n",
    "    - **Incorporation des données macroéconomiques** : Ajouter des indicateurs macroéconomiques disponible grâce à l'API tels que le ``taux de chômage``, ``inflation`` (TIPS and CPI), et les ``taux d'intérêt`` pour améliorer les prédictions.\n",
    "    - **Analyse comparative des modèles** : De la classification à la regression (logistique). Tester différents types de boosted trees, puis voire si l'implémentation de RNN comme LSTM pour prendre en compte les relations de dépendence temporelle des time series permet d'ameliorer les performances.\n",
    "    - **Analyse de la saisonnalité** : Étudier les effets saisonniers sur les prix de l'or et la dette américaine -> faire varier l'échelle des données (jusque-là on travaillait avec du daily mais comme on l'a vu dans la partie Granger Causality, il peut être intéressant de revenir à du ``weekly`` ou du ``monthly``) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable selection with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données\n",
    "X = data.drop(columns=['debt_delta', 'target', 'debt_amt'])\n",
    "y = data['debt_delta']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Définir les valeurs possibles pour alpha (échelle logarithmique)\n",
    "alphas = np.logspace(-1, 5, 30)\n",
    "\n",
    "# Initialiser le modèle Ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "# Utiliser GridSearchCV pour trouver le meilleur alpha\n",
    "param_grid = {'alpha': alphas}\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Meilleur alpha trouvé\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"Meilleur alpha trouvé: {best_alpha}\")\n",
    "\n",
    "# Entraîner le modèle Ridge avec le meilleur alpha\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Sélectionner les caractéristiques basées sur le modèle Ridge\n",
    "selector = SelectFromModel(ridge_best, prefit=True)\n",
    "X_train_selected = selector.transform(X_train_scaled)\n",
    "\n",
    "# Plot de l'évolution de la significativité des régressions\n",
    "coefs = []\n",
    "for alpha in alphas:\n",
    "    ridge.set_params(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.log10(alphas), coefs)\n",
    "plt.xlabel('log10(alpha)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Evolution des coefficients en fonction de log(alpha)')\n",
    "plt.axis('tight')\n",
    "plt.legend(X.columns)\n",
    "plt.show()\n",
    "\n",
    "# Afficher les caractéristiques sélectionnées\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Caractéristiques sélectionnées:\", selected_features)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "y_pred = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "# Calcul de l'erreur quadratique moyenne\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(f\"Erreur quadratique moyenne: {mse}\")\n",
    "\n",
    "# Evalualuation du modèle Ridge\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test.index, y_test, label='True')\n",
    "plt.scatter(y_test.index, y_pred, label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Debt Delta')\n",
    "plt.title('Prédiction du modèle Ridge')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection with Lasso and Gradient Boosting fitting in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "      return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X['gold_vol7d'] = X['gold_return'].rolling(window=7).std() * ((365/7)**0.5)\n",
    "        X['gold_vol28d'] = X['gold_return'].rolling(window=28).std() * ((365/28)**0.5)\n",
    "        X['gold_vol90d'] = X['gold_return'].rolling(window=90).std() * ((365/90)**0.5)\n",
    "        X['gold_volume_variation'] = X['gold_volume'].diff().fillna(0)\n",
    "        X['gold_variation'] = X['gold_price'].diff()\n",
    "        X['gold_volume_absolut_variation'] = abs(X['gold_volume_variation'])\n",
    "        X['gold_volume_absolut_variation_7d'] = X['gold_volume_absolut_variation'].rolling(window=7).sum()\n",
    "        X['gold_volume_absolut_variation_28d'] = X['gold_volume_absolut_variation'].rolling(window=28).sum()\n",
    "        X['gold_volume_absolut_variation_90d'] = X['gold_volume_absolut_variation'].rolling(window=90).sum()\n",
    "        X.dropna(inplace=True)\n",
    "        y = y.loc[X.index]     \n",
    "        return X,y\n",
    "\n",
    "class ExpandingWindowCV(BaseCrossValidator):\n",
    "    def __init__(self, initial_train_size, test_size, total_size):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - initial_train_size: int, size of the initial training set\n",
    "        - test_size: int, size of the test set for each split\n",
    "        - total_size: int, total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        self.initial_train_size = initial_train_size\n",
    "        self.test_size = test_size\n",
    "        self.total_size = total_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Yields train and test indices for each split.\n",
    "        \"\"\"\n",
    "        n_splits = (self.total_size - self.initial_train_size) // self.test_size\n",
    "        for i in range(n_splits):\n",
    "            train_start = 0\n",
    "            train_end = self.initial_train_size + i * self.test_size\n",
    "            test_start = train_end\n",
    "            test_end = test_start + self.test_size\n",
    "            yield np.arange(train_start, train_end), np.arange(test_start, test_end)\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return (self.total_size - self.initial_train_size) // self.test_size\n",
    "\n",
    "# Définition d'une matrice de coût pour la classification (se tromper sur la paire endettement/surendettement est moins grave que si remboursement)\n",
    "COST_MATRIX = np.array([\n",
    "    [0, 10, 10],  # True label 0\n",
    "    [10, 0, 1],   # True label 1\n",
    "    [10, 1, 0]    # True label 2\n",
    "])\n",
    "\n",
    "def cost_sensitive_metric(y_true, y_pred,cost_matrix=COST_MATRIX):\n",
    "    \"\"\"\n",
    "    Custom cost-sensitive metric based on a cost matrix\n",
    "    \"\"\"\n",
    "    cost = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        cost += cost_matrix[int(true), int(pred)]\n",
    "    return -cost / len(y_true)  # Negative because GridSearchCV maximizes the scorer\n",
    "\n",
    "# Create a scorer\n",
    "cost_scorer = make_scorer(cost_sensitive_metric, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement & pré traitement des données\n",
    "gold_data = gold_data_2[['Price', 'Volume', 'Daily_return']].rename(columns={'Price': 'gold_price', 'Volume': 'gold_volume', 'Daily_return': 'gold_return'})\n",
    "debt_data = debt_to_penny_df[['tot_pub_debt_out_amt']].rename(columns={'tot_pub_debt_out_amt': 'debt_amt'})\n",
    "data = pd.merge(gold_data, debt_data, left_index=True, right_index=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "x = 30 \n",
    "q = 0.95 \n",
    "\n",
    "data['debt_delta'] = (data['debt_amt'].shift(-x) - data['debt_amt']) / data['debt_amt']\n",
    "s = data['debt_delta'].quantile(q)\n",
    "data['target'] = pd.cut(data['debt_delta'], bins=[-np.inf, 0, s, np.inf], labels=['remboursement', 'endettement', 'surendettement'])\n",
    "data.dropna(subset=['target'], inplace=True)\n",
    "\n",
    "X = data.drop(columns=['debt_delta', 'target', 'debt_amt'])\n",
    "y = data[['target']]\n",
    "mapping = {'endettement': 1.0, 'surendettement': 2.0, 'remboursement': -1.0} \n",
    "y = y['target'].map(mapping).astype(float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X.iloc[:int(len(X)*0.8)], X.iloc[int(len(X)*0.8):], y.iloc[:int(len(y)*0.8)], y.iloc[int(len(y)*0.8):]\n",
    "X_train, y_train = CustomFeatureEngineering().transform(X_train, y_train)\n",
    "X_test, y_test = CustomFeatureEngineering().transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        #(\"feature_engineering\",CustomFeatureEngineering()),\n",
    "        ('resampling', SMOTE(random_state=42)),\n",
    "        (\"scaling\",StandardScaler()),\n",
    "        ('feature_selection', SelectFromModel(Ridge(random_state=42))),\n",
    "        (\"xgboost\",GradientBoostingClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "param_grid = {\n",
    "    'feature_selection__estimator__alpha': [1],\n",
    "    'xgboost__n_estimators': [300],\n",
    "    'xgboost__max_depth': [5],\n",
    "    'xgboost__learning_rate': [0.3],# 0.2],\n",
    "    'xgboost__subsample': [0.85],\n",
    "}\n",
    "\n",
    "initial_train_size = 1000\n",
    "test_size = 500\n",
    "cv = ExpandingWindowCV(initial_train_size=initial_train_size, test_size=test_size, total_size=len(X_train))\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, refit=True, verbose=3, n_jobs=-1) #,scoring=cost_scorer)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "#print(\"Best Parameters:\", grid_search.best_params_)\n",
    "#print(\"Best CV Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid_search.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test.values, grid_search.predict(X_test))\n",
    "display = ConfusionMatrixDisplay(cf_matrix, display_labels=[\"remboursement\",\"endettement\",\"surendettement\"])\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des features et de la target\n",
    "X = data[selected_features]\n",
    "y = data['target']\n",
    "\n",
    "# Entraînement d'un modèle de classification de type Gradient Boosting avec GridSearchCV\n",
    "# Séparation des données\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définition des hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialisation du modèle\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialisation de GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Entraînement du modèle\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "y_pred = grid_search.predict(X_test)\n",
    "y_proba = grid_search.predict_proba(X_test)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres\n",
    "print(f'Les meilleurs hyperparamètres sont: {grid_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-Search like function to Fine Tune the model with ``x`` and ``q``\n",
    "Ici on fine tune le model à travers une selection de features faite par un ridge :\n",
    "- Pour chaque (`x`, `q`) on redefinit la nouvelle target en recalculant `debt_amt` et `s`\n",
    "- On fine tune avec un ``gridSearchCV`` un ``ridge optimal`` au sens du coefficient de shrink\n",
    "- on selectionne des features avec ce ridge optimal à l'aide de ``SelectFromModel``\n",
    "- on prédit les données avec le ridge optimal sur les features selectionnées\n",
    "- On retien le couple (`x`, `q`) fournissant le meilleur ``r2_score``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Regression model pipeline with a StandardScaler,  SelectFromModel Ridge then a regression model\n",
    "\n",
    "def Regression_Model(x, q, data):\n",
    "    # Target creation\n",
    "    data = DebtDeltaTransformer(x, q).transform(data)\n",
    "\n",
    "    # param_grid \n",
    "    param_grid={'feature_selection__estimator__alpha': np.logspace(-1, 5, 30)}\n",
    "    # Pipeline for the regression task\n",
    "    model_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectFromModel(Ridge())),\n",
    "        ('regressor', LinearRegression()),\n",
    "    ])\n",
    "\n",
    "    # Initialiser GridSearchCV avec TimeSeriesSplit\n",
    "    grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=TimeSeriesSplit(n_splits=5))\n",
    "\n",
    "    # Ajuster GridSearchCV aux données\n",
    "    grid_search.fit(data.drop(columns=['debt_delta', 'target']), data['debt_delta'])\n",
    "\n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid-Search like function to Fine Tune the model with x and q\n",
    "param_grid = {\n",
    "    'x': [1, 7, 28, 90],\n",
    "    'q': [0.9, 0.95, .97, 0.99]\n",
    "}\n",
    "\n",
    "def fine_tune_model(param_grid, verbose=False):\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_features = None\n",
    "    for x in param_grid['x']:\n",
    "        for q in param_grid['q']:\n",
    "            # Calcul de la variation relative de la dette sur x jours\n",
    "            data['debt_delta'] = (data['debt_amt'].shift(-x) - data['debt_amt']) / data['debt_amt']\n",
    "\n",
    "            # Calcul du seuil s\n",
    "            s = data['debt_delta'].quantile(q)\n",
    "\n",
    "            # Classification de la target en 3 classes\n",
    "            data['target'] = pd.cut(data['debt_delta'], bins=[-np.inf, 0, s, np.inf], labels=['remboursement', 'endettement', 'surendettement'])\n",
    "\n",
    "            # Suppression des lignes avec des valeurs manquantes dans la target\n",
    "            data.dropna(subset=['target'], inplace=True)\n",
    "\n",
    "            # Séparation des données X contenant les features et y contenant la target\n",
    "            X = data.drop(columns=['debt_delta', 'target'])\n",
    "            y = data['debt_delta']\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Regression Ridge pour selectionner les features\n",
    "\n",
    "            # Standardiser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Initialiser le modèle Ridge\n",
    "            ridge = Ridge()\n",
    "\n",
    "            # Utiliser GridSearchCV pour trouver le meilleur alpha\n",
    "            param_grid_ridge = {'alpha': alphas}\n",
    "            grid_search_ridge = GridSearchCV(ridge, param_grid_ridge, cv=5)\n",
    "            grid_search_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Meilleur alpha trouvé\n",
    "            best_alpha = grid_search_ridge.best_params_['alpha']\n",
    "\n",
    "            # Entraîner le modèle Ridge avec le meilleur alpha\n",
    "            ridge_best = Ridge(alpha=best_alpha)\n",
    "            ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Sélectionner les caractéristiques basées sur le modèle Ridge\n",
    "            selector = SelectFromModel(ridge_best, prefit=True)\n",
    "            X_train_selected = selector.transform(X_train_scaled)\n",
    "\n",
    "            # Plot de l'évolution de la significativité des régressions\n",
    "            if verbose:\n",
    "                coefs = []\n",
    "                for alpha in alphas:\n",
    "                    ridge.set_params(alpha=alpha)\n",
    "                    ridge.fit(X_train_scaled, y_train)\n",
    "                    coefs.append(ridge.coef_)\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(np.log10(alphas), coefs)\n",
    "                plt.xlabel('log10(alpha)')\n",
    "                plt.ylabel('Coefficients')\n",
    "                plt.title(f'Evolution des coefficients en fonction de log(alpha) pour x={x} et q={q}')\n",
    "                plt.axis('tight')\n",
    "                plt.legend(X.columns)\n",
    "                plt.show()\n",
    "            \n",
    "            # Afficher les caractéristiques sélectionnées\n",
    "            selected_features = X.columns[selector.get_support()]\n",
    "            if verbose:\n",
    "                print(f'Caractéristiques sélectionnées pour x={x} et q={q}: {selected_features}')\n",
    "\n",
    "            # Evaluation du modèle Ridge\n",
    "            X_test_selected = selector.transform(X_test_scaled)\n",
    "            y_pred = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "            # Compute the score model (here R2 score but can be changed to another metric such as mean_absolute_percentage_error)\n",
    "            score = r2_score(y_test, y_pred)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Score for x={x} and q={q}: {score}')\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'x': x, 'q': q}\n",
    "                best_features = selected_features\n",
    "    \n",
    "    return best_params, best_score, best_features\n",
    "\n",
    "# Fine tune the model\n",
    "\n",
    "best_params, best_score, best_features = fine_tune_model(param_grid, verbose=True)\n",
    "print(f\"Meilleurs paramètres trouvés: {best_params}\")\n",
    "print(f\"Meilleure score: {best_score}\")\n",
    "print(f\"Meilleures caractéristiques sélectionnées: {best_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Strategy\n",
    "---\n",
    "On a un modèle qui nous permet de detecter des signaux concernant les variations futures (horizon `x` jours) de la dette américaine, il ne nous reste plus qu'à trouver une manière d'en tirer profit avec un stratégie de trading. Deux questions : Quoi trader ? Comment charger les positions ?\n",
    "\n",
    "- **Proxy de la dette américaine** : verfier si on peut encoder la pente de la dette US par le ``spread d'un swap taux US 10y - 1y equi-std`` comme instrument pour notre stratégie de trading (donnée des swaps rates accessible ici : https://fred.stlouisfed.org/categories/32299)\n",
    "- **Construction d'une stratégie basée sur le modèle de classification** : \n",
    "    - On regarde le signal donnée par le modèle de classification optimisé pour savoir si on long ou short\n",
    "    - Pour charger les positions on peut utiliser la methode ``predict_proba`` pour avoir les probas de notre estimateur de classification\n",
    "- **Backtesting des stratégies de trading** : Mettre en place un environnement de backtesting pour évaluer les performances des stratégies de trading basées sur les modèles prédictifs et sur un proxy de la dette américaine comme instrument tradable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1y</th>\n",
       "      <th>3y</th>\n",
       "      <th>5y</th>\n",
       "      <th>10y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-07-03</th>\n",
       "      <td>7.10</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-05</th>\n",
       "      <td>7.03</td>\n",
       "      <td>7.07</td>\n",
       "      <td>7.08</td>\n",
       "      <td>7.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-06</th>\n",
       "      <td>7.07</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.16</td>\n",
       "      <td>7.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-07</th>\n",
       "      <td>7.01</td>\n",
       "      <td>7.06</td>\n",
       "      <td>7.07</td>\n",
       "      <td>7.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-24</th>\n",
       "      <td>0.99</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-25</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-26</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-27</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-28</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4260 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1y    3y    5y   10y\n",
       "date                              \n",
       "2000-07-03  7.10  7.17  7.17  7.24\n",
       "2000-07-04   NaN   NaN   NaN   NaN\n",
       "2000-07-05  7.03  7.07  7.08  7.14\n",
       "2000-07-06  7.07  7.14  7.16  7.21\n",
       "2000-07-07  7.01  7.06  7.07  7.14\n",
       "...          ...   ...   ...   ...\n",
       "2016-10-24  0.99  1.15  1.30  1.61\n",
       "2016-10-25  1.00  1.15  1.29  1.59\n",
       "2016-10-26  1.01  1.18  1.33  1.64\n",
       "2016-10-27  1.00  1.20  1.38  1.71\n",
       "2016-10-28  1.00  1.19  1.36  1.71\n",
       "\n",
       "[4260 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your FRED API Key\n",
    "API_KEY = \"\"\n",
    "\n",
    "series_ids = {\n",
    "    \"10y\": \"DSWP10\", \n",
    "    \"1y\": \"DSWP1\",\n",
    "    \"3y\": \"DSWP3\",\n",
    "    \"5y\": \"DSWP5\",\n",
    "}\n",
    "\n",
    "# Base URL for FRED API\n",
    "BASE_URL = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "\n",
    "def fetch_swaps_data(series_id):\n",
    "    \"\"\"Fetch data from FRED API for a given series ID.\"\"\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"api_key\": API_KEY,\n",
    "        \"file_type\": \"json\"\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()[\"observations\"]\n",
    "        df = pd.DataFrame(data)\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")  \n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]) \n",
    "        return df[[\"date\", \"value\"]]\n",
    "    else:\n",
    "        print(f\"Error fetching data for series {series_id}: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "data_1y = fetch_swaps_data(series_ids[\"1y\"])\n",
    "data_3y = fetch_swaps_data(series_ids[\"3y\"])\n",
    "data_5y = fetch_swaps_data(series_ids[\"5y\"])\n",
    "data_10y = fetch_swaps_data(series_ids[\"10y\"])\n",
    "\n",
    "data_frames = [data_1y, data_3y, data_5y, data_10y]\n",
    "swaps_data = reduce(lambda  left,right: pd.merge(left,right,on=['date'],\n",
    "                                            how='outer'), data_frames)\n",
    "swaps_data.columns = ['date', '1y', '3y', '5y', '10y']\n",
    "swaps_data.set_index('date', inplace=True)\n",
    "swaps_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_bonds[[\"US1M\",\"US6M\",\"US1Y\",\"US3Y\"]].loc[\"1980-01-01\":].plot()\n",
    "plt.title('US Bonds Yields')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Yields in %')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for column in us_bonds.columns:\n",
    "    us_bonds[f'{column}_delta'] = (us_bonds[column].shift(-x) - us_bonds[column]) / us_bonds[column]\n",
    "    \n",
    "us_bonds_target_train = us_bonds.merge(pd.DataFrame(y_train),left_index=True,right_index=True)\n",
    "\n",
    "{col: us_bonds_target_train[\"target\"].corr(us_bonds_target_train[col]) for col in us_bonds_target_train.columns if col != \"target\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
